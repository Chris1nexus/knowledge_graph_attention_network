{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a047b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility.metrics as metrics\n",
    "from utility.parser import parse_args\n",
    "import multiprocessing\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "#from utility.loader_bprmf import BPRMF_loader\n",
    "\n",
    "#from utility.loader_cke import CKE_loader\n",
    "#from utility.loader_nfm import NFM_loader\n",
    "from utility.loader_kgat import KGATDataset\n",
    "#from utility.loader_cfkg import CFKG_loader\n",
    "\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "    parser.add_argument('--weights_path', nargs='?', default='',\n",
    "                        help='Store model path.')\n",
    "    parser.add_argument('--data_path', nargs='?', default='../Data/',\n",
    "                        help='Input data path.')\n",
    "    parser.add_argument('--proj_path', nargs='?', default='',\n",
    "                        help='Project path.')\n",
    "\n",
    "    parser.add_argument('--dataset', nargs='?', default='last-fm',\n",
    "                        help='Choose a dataset from {yelp2018, last-fm, amazon-book}')\n",
    "    parser.add_argument('--pretrain', type=int, default=0,\n",
    "                        help='0: No pretrain, -1: Pretrain with the learned embeddings, 1:Pretrain with stored models.')\n",
    "    parser.add_argument('--verbose', type=int, default=1,\n",
    "                        help='Interval of evaluation.')\n",
    "    parser.add_argument('--epoch', type=int, default=100,\n",
    "                        help='Number of epoch.')\n",
    "\n",
    "    parser.add_argument('--embed_size', type=int, default=64,\n",
    "                        help='CF Embedding size.')\n",
    "    parser.add_argument('--kge_size', type=int, default=64,\n",
    "                        help='KG Embedding size.')\n",
    "    parser.add_argument('--layer_size', nargs='?', default='[64]',\n",
    "                        help='Output sizes of every layer')\n",
    "\n",
    "    parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--batch_size_kg', type=int, default=2048,\n",
    "                        help='KG batch size.')\n",
    "\n",
    "    parser.add_argument('--regs', nargs='?', default='[1e-5,1e-5,1e-2]',\n",
    "                        help='Regularization for user and item embeddings.')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                        help='Learning rate.')\n",
    "\n",
    "    parser.add_argument('--model_type', nargs='?', default='kgat',\n",
    "                        help='Specify a loss type from {kgat, bprmf, fm, nfm, cke, cfkg}.')\n",
    "    parser.add_argument('--adj_type', nargs='?', default='si',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {bi, si}.')\n",
    "    parser.add_argument('--alg_type', nargs='?', default='ngcf',\n",
    "                        help='Specify the type of the graph convolutional layer from {bi, gcn, graphsage}.')\n",
    "    parser.add_argument('--adj_uni_type', nargs='?', default='sum',\n",
    "                        help='Specify a loss type (uni, sum).')\n",
    "\n",
    "    parser.add_argument('--gpu_id', type=int, default=0,\n",
    "                        help='0 for NAIS_prod, 1 for NAIS_concat')\n",
    "\n",
    "    parser.add_argument('--node_dropout', nargs='?', default='[0.1]',\n",
    "                        help='Keep probability w.r.t. node dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1]',\n",
    "                        help='Keep probability w.r.t. message dropout (i.e., 1-dropout_ratio) for each deep layer. 1: no dropout.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[20, 40, 60, 80, 100]',\n",
    "                        help='Output sizes of every layer')\n",
    "\n",
    "    parser.add_argument('--save_flag', type=int, default=0,\n",
    "                        help='0: Disable model saver, 1: Activate model saver')\n",
    "\n",
    "    parser.add_argument('--test_flag', nargs='?', default='part',\n",
    "                        help='Specify the test type from {part, full}, indicating whether the reference is done in mini-batch')\n",
    "\n",
    "    parser.add_argument('--report', type=int, default=0,\n",
    "                        help='0: Disable performance report w.r.t. sparsity levels, 1: Show performance report w.r.t. sparsity levels')\n",
    "\n",
    "    parser.add_argument('--use_att', type=bool, default=True,\n",
    "                        help='whether using attention mechanism')\n",
    "    parser.add_argument('--use_kge', type=bool, default=True,\n",
    "                        help='whether using knowledge graph embedding')\n",
    "    \n",
    "    parser.add_argument('--l1_flag', type=bool, default=True,\n",
    "                        help='Flase: using the L2 norm, True: using the L1 norm.')\n",
    "\n",
    "    return parser.parse_args(args=[])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875ac161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[23566, 48123]\n",
      "[n_train, n_test]=[1289003, 423635]\n",
      "[n_entities, n_relations, n_triples]=[106389, 9, 464567]\n",
      "[batch_size, batch_size_kg]=[1024, 369]\n",
      "\tconvert ratings into adj mat done.\n",
      "\tconvert 20 relational triples into adj mat done. @0.4836s\n",
      "\tgenerate si-normalized adjacency matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xrh1/experiments/kgat/kgat_mod_repo/knowledge_graph_attention_network/Model/utility/loader_kgat.py:94: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treordering indices...\n",
      "\treorganize all kg data done.\n",
      "\tsort meta-data done.\n",
      "\tsort all data done.\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count() // 2\n",
    "\n",
    "args = parse_args()\n",
    "Ks = eval(args.Ks)\n",
    "\n",
    "if args.model_type == 'bprmf':\n",
    "    data_generator = BPRMF_loader(args=args, path=args.data_path + args.dataset)\n",
    "    batch_test_flag = False\n",
    "\n",
    "elif args.model_type == 'cke':\n",
    "    data_generator = CKE_loader(args=args, path=args.data_path + args.dataset)\n",
    "    batch_test_flag = False\n",
    "\n",
    "elif args.model_type in ['cfkg']:\n",
    "    data_generator = CFKG_loader(args=args, path=args.data_path + args.dataset)\n",
    "    batch_test_flag = True\n",
    "\n",
    "elif args.model_type in ['fm','nfm']:\n",
    "    data_generator = NFM_loader(args=args, path=args.data_path + args.dataset)\n",
    "    batch_test_flag = True\n",
    "\n",
    "elif args.model_type in ['kgat']:\n",
    "    data_generator = KGATDataset(args=args, path=args.data_path + args.dataset)\n",
    "    batch_test_flag = False\n",
    "\n",
    "\n",
    "USR_NUM, ITEM_NUM = data_generator.n_users, data_generator.n_items\n",
    "N_TRAIN, N_TEST = data_generator.n_train, data_generator.n_test\n",
    "BATCH_SIZE = args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62afe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utility.loader_kgat.KGAT_loader at 0x7f93457d0670>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4d4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "l =[]\n",
    "\n",
    "for i in range(100000):\n",
    "    l\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end-start)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "022fbbd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keyiterator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_194571/3970047408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmydict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmydict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keyiterator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "mydict = {'a':1, 'b':2, 'c':3}\n",
    "\n",
    "iter(mydict.keys())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "98e1d7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "random.sample(mydict.keys(), 2)\n",
    "\n",
    "a1 = (1,2)\n",
    "a2 = (3,4)\n",
    "\n",
    "a3 = (*a1, *a2)\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "bf9fb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "import math\n",
    "class IterRecomDataset(IterableDataset):\n",
    "    \n",
    "    def __init__(self, args, path):\n",
    "        super(RecomDataset).__init__()\n",
    "        self.path = path\n",
    "        self.args = args\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        kg_file = path + '/kg_final.txt'\n",
    "\n",
    "        # ----------get number of users and items & then load rating data from train_file & test_file------------.\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "\n",
    "        self.train_data, self.train_user_dict = self._load_ratings(train_file)\n",
    "        self.test_data, self.test_user_dict = self._load_ratings(test_file)\n",
    "        self.exist_users = list(self.train_user_dict.keys())\n",
    "        self.N_exist_users = len(self.exist_users)\n",
    "        \n",
    "        self._statistic_ratings()\n",
    "\n",
    "        # ----------get number of entities and relations & then load kg data from kg_file ------------.\n",
    "        self.n_relations, self.n_entities, self.n_triples = 0, 0, 0\n",
    "        self.kg_data, self.kg_dict, self.relation_dict = self._load_kg(kg_file)\n",
    "\n",
    "        # ----------print the basic info about the dataset-------------.\n",
    "        self.batch_size_kg = self.n_triples // (self.n_train // self.batch_size)\n",
    "        self._print_data_info()\n",
    "    \n",
    "    # reading train & test interaction data.\n",
    "    def _load_ratings(self, file_name):\n",
    "        user_dict = dict()\n",
    "        inter_mat = list()\n",
    "\n",
    "        lines = open(file_name, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmps = l.strip()\n",
    "            inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "            u_id, pos_ids = inters[0], inters[1:]\n",
    "            pos_ids = list(set(pos_ids))\n",
    "\n",
    "            for i_id in pos_ids:\n",
    "                inter_mat.append([u_id, i_id])\n",
    "\n",
    "            if len(pos_ids) > 0:\n",
    "                user_dict[u_id] = pos_ids\n",
    "        return np.array(inter_mat), user_dict\n",
    "\n",
    "    def _statistic_ratings(self):\n",
    "        self.n_users = max(max(self.train_data[:, 0]), max(self.test_data[:, 0])) + 1\n",
    "        self.n_items = max(max(self.train_data[:, 1]), max(self.test_data[:, 1])) + 1\n",
    "        self.n_train = len(self.train_data)\n",
    "        self.n_test = len(self.test_data)\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_kg(self, file_name):\n",
    "        def _construct_kg(kg_np):\n",
    "            kg = collections.defaultdict(list)\n",
    "            rd = collections.defaultdict(list)\n",
    "\n",
    "            for head, relation, tail in kg_np:\n",
    "                kg[head].append((tail, relation))\n",
    "                rd[relation].append((head, tail))\n",
    "            return kg, rd\n",
    "\n",
    "        kg_np = np.loadtxt(file_name, dtype=np.int32)\n",
    "        kg_np = np.unique(kg_np, axis=0)\n",
    "\n",
    "        # self.n_relations = len(set(kg_np[:, 1]))\n",
    "        # self.n_entities = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        self.n_relations = max(kg_np[:, 1]) + 1\n",
    "        self.n_entities = max(max(kg_np[:, 0]), max(kg_np[:, 2])) + 1\n",
    "        self.n_triples = len(kg_np)\n",
    "\n",
    "        kg_dict, relation_dict = _construct_kg(kg_np)\n",
    "\n",
    "        return kg_np, kg_dict, relation_dict\n",
    "\n",
    "    def _print_data_info(self):\n",
    "        print('[n_users, n_items]=[%d, %d]' % (self.n_users, self.n_items))\n",
    "        print('[n_train, n_test]=[%d, %d]' % (self.n_train, self.n_test))\n",
    "        print('[n_entities, n_relations, n_triples]=[%d, %d, %d]' % (self.n_entities, self.n_relations, self.n_triples))\n",
    "        print('[batch_size, batch_size_kg]=[%d, %d]' % (self.batch_size, self.batch_size_kg))\n",
    "\n",
    "\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_user_dict.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_user_dict[uid]\n",
    "            test_iids = self.test_user_dict[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "    \n",
    "    def __iter__(self):        \n",
    "        \"\"\"\n",
    "        if self.batch_size <= self.n_users:\n",
    "            user = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "        \"\"\"\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "\n",
    "        # single-process data loading, return the full iterator\n",
    "        iter_start = 0\n",
    "        iter_end = self.N_exist_users #len(self.exist_users)\n",
    "        \n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "\n",
    "        if worker_info is not None:  # in a worker process\n",
    "            # split workload\n",
    "            start = iter_start\n",
    "            end = iter_end\n",
    "            per_worker = int(math.ceil((end - start) / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            iter_start = start + worker_id * per_worker\n",
    "            iter_end = min(iter_start + per_worker, end)\n",
    "            \n",
    "        #worker_user_subset = self.exist_users[iter_start:iter_end]\n",
    "        if self.batch_size <= (iter_end-iter_start):#len(worker_user_subset):#self.n_users:\n",
    "\n",
    "            users = [self.exist_users[idx]  for idx in rd.sample(range(iter_start, iter_end), \n",
    "                              self.batch_size)]\n",
    "        else:\n",
    "            user_range = range(iter_start, iter_end)\n",
    "            users = [self.exist_users[rd.choice(user_range)] for _ in range(self.batch_size)]            \n",
    "            \n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "            \n",
    "        return zip(users, pos_items, neg_items)    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "class RecomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, args, path):\n",
    "        super(RecomDataset).__init__()\n",
    "        self.path = path\n",
    "        self.args = args\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        kg_file = path + '/kg_final.txt'\n",
    "\n",
    "        # ----------get number of users and items & then load rating data from train_file & test_file------------.\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "\n",
    "        self.train_data, self.train_user_dict = self._load_ratings(train_file)\n",
    "        self.test_data, self.test_user_dict = self._load_ratings(test_file)\n",
    "        self.exist_users = list(self.train_user_dict.keys())\n",
    "        self.N_exist_users = len(self.exist_users)\n",
    "        \n",
    "        self._statistic_ratings()\n",
    "\n",
    "        # ----------get number of entities and relations & then load kg data from kg_file ------------.\n",
    "        self.n_relations, self.n_entities, self.n_triples = 0, 0, 0\n",
    "        self.kg_data, self.kg_dict, self.relation_dict = self._load_kg(kg_file)\n",
    "\n",
    "        # ----------print the basic info about the dataset-------------.\n",
    "        self.batch_size_kg = self.n_triples // (self.n_train // self.batch_size)\n",
    "        self._print_data_info()\n",
    "    \n",
    "    # reading train & test interaction data.\n",
    "    def _load_ratings(self, file_name):\n",
    "        user_dict = dict()\n",
    "        inter_mat = list()\n",
    "\n",
    "        lines = open(file_name, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmps = l.strip()\n",
    "            inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "            u_id, pos_ids = inters[0], inters[1:]\n",
    "            pos_ids = list(set(pos_ids))\n",
    "\n",
    "            for i_id in pos_ids:\n",
    "                inter_mat.append([u_id, i_id])\n",
    "\n",
    "            if len(pos_ids) > 0:\n",
    "                user_dict[u_id] = pos_ids\n",
    "        return np.array(inter_mat), user_dict\n",
    "\n",
    "    def _statistic_ratings(self):\n",
    "        self.n_users = max(max(self.train_data[:, 0]), max(self.test_data[:, 0])) + 1\n",
    "        self.n_items = max(max(self.train_data[:, 1]), max(self.test_data[:, 1])) + 1\n",
    "        self.n_train = len(self.train_data)\n",
    "        self.n_test = len(self.test_data)\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_kg(self, file_name):\n",
    "        def _construct_kg(kg_np):\n",
    "            kg = collections.defaultdict(list)\n",
    "            rd = collections.defaultdict(list)\n",
    "\n",
    "            for head, relation, tail in kg_np:\n",
    "                kg[head].append((tail, relation))\n",
    "                rd[relation].append((head, tail))\n",
    "            return kg, rd\n",
    "\n",
    "        kg_np = np.loadtxt(file_name, dtype=np.int32)\n",
    "        kg_np = np.unique(kg_np, axis=0)\n",
    "\n",
    "        # self.n_relations = len(set(kg_np[:, 1]))\n",
    "        # self.n_entities = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        self.n_relations = max(kg_np[:, 1]) + 1\n",
    "        self.n_entities = max(max(kg_np[:, 0]), max(kg_np[:, 2])) + 1\n",
    "        self.n_triples = len(kg_np)\n",
    "\n",
    "        kg_dict, relation_dict = _construct_kg(kg_np)\n",
    "\n",
    "        return kg_np, kg_dict, relation_dict\n",
    "\n",
    "    def _print_data_info(self):\n",
    "        print('[n_users, n_items]=[%d, %d]' % (self.n_users, self.n_items))\n",
    "        print('[n_train, n_test]=[%d, %d]' % (self.n_train, self.n_test))\n",
    "        print('[n_entities, n_relations, n_triples]=[%d, %d, %d]' % (self.n_entities, self.n_relations, self.n_triples))\n",
    "        print('[batch_size, batch_size_kg]=[%d, %d]' % (self.batch_size, self.batch_size_kg))\n",
    "\n",
    "\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_user_dict.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_user_dict[uid]\n",
    "            test_iids = self.test_user_dict[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "\n",
    "        return split_uids, split_state\n",
    "    def __len__(self):\n",
    "        # number of existing users after the preprocessing described in the paper, \n",
    "        # determines the length of the training dataset, for which a positive an negative are extracted\n",
    "        return len(self.exist_users)\n",
    "    \n",
    "    ##_generate_train_cf_batch\n",
    "    def __getitem__(self, idx):      \n",
    "        \"\"\"\n",
    "        if self.batch_size <= self.n_users:\n",
    "            user = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "        \"\"\"\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "        \"\"\"\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "        \"\"\"\n",
    "        u = self.exist_users[idx]\n",
    "        pos_item = sample_pos_items_for_u(u, 1)\n",
    "        neg_item = sample_neg_items_for_u(u, 1)\n",
    "        if len(pos_item) == 1:\n",
    "            pos_item = pos_item[0]\n",
    "        if len(neg_item) == 1:\n",
    "            neg_item = neg_item[0]            \n",
    "        return u, pos_item, neg_item #users, pos_items, neg_items\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a034f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7264b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab25ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8a88499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002822399139404297\n",
      "0.001249074935913086\n",
      "0.0036821365356445312\n"
     ]
    }
   ],
   "source": [
    "# Single-process loading\n",
    "\n",
    "loader1 = iter(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=0))\n",
    "loader2 = iter(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=2))\n",
    "loader3 = iter(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=32))\n",
    "\n",
    "start = time.time()\n",
    "#print(list(DataLoader(iter_ds, num_workers=0)))\n",
    "next(loader1)\n",
    "end = time.time()    \n",
    "print(end-start)\n",
    "\n",
    "# Mult-process loading with two worker processes\n",
    "# Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n",
    "start = time.time()\n",
    "#print(list(DataLoader(iter_ds, num_workers=2)))\n",
    "next(loader2)\n",
    "end = time.time()    \n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "# With even more workers\n",
    "#print(list(DataLoader(iter_ds, num_workers=20)))\n",
    "next(loader3)\n",
    "end = time.time()    \n",
    "print(end-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faafbd5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_633108/3390298521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecomDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRecomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "from utility.load_data import RecomDataset\n",
    "ds = RecomDataset(args=args, path=args.data_path + args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1071a3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[5941, 10303]\n",
      "[n_train, n_test]=[738345, 217212]\n",
      "[n_entities, n_relations, n_triples]=[57541, 8, 90778]\n",
      "[batch_size, batch_size_kg]=[1024, 125]\n",
      "[n_users, n_items]=[5941, 10303]\n",
      "[n_train, n_test]=[738345, 217212]\n",
      "[n_entities, n_relations, n_triples]=[57541, 8, 90778]\n",
      "[batch_size, batch_size_kg]=[1024, 125]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = RecomDataset(args=args, path=args.data_path + args.dataset)\n",
    "\n",
    "iter_ds = IterRecomDataset(args=args, path=args.data_path + args.dataset)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b39811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "019c01b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[5941, 10303]\n",
      "[n_train, n_test]=[738345, 217212]\n",
      "[n_entities, n_relations, n_triples]=[57541, 8, 90778]\n",
      "[batch_size, batch_size_kg]=[1024, 125]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1647, 1732)"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BPRMF_loader(RecomDataset):\n",
    "    def __init__(self, args, path):\n",
    "        super().__init__(args, path)\n",
    "\n",
    "\n",
    "    def as_test_feed_dict(self, model, user_batch, item_batch, drop_flag=True):\n",
    "\n",
    "        feed_dict = {\n",
    "            model.users: user_batch,\n",
    "            model.pos_items: item_batch\n",
    "        }\n",
    "\n",
    "        return feed_dict  \n",
    "    def as_train_feed_dict(self, model, users, pos_items, neg_items):\n",
    "        batch_data = {}\n",
    "        batch_data['users'] = users\n",
    "        batch_data['pos_items'] = pos_items\n",
    "        batch_data['neg_items'] = neg_items\n",
    "        feed_dict = {\n",
    "            model.users: batch_data['users'],\n",
    "            model.pos_items: batch_data['pos_items'],\n",
    "            model.neg_items: batch_data['neg_items']\n",
    "        }\n",
    "\n",
    "        return feed_dict   \n",
    "bprmf = BPRMF_loader(args=args, path=args.data_path + args.dataset)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d5687774",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import numpy as np\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for x in iter(dl): yield x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8577cf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([780, 794, 908,  ..., 807, 844, 814]),\n",
       " tensor([3753,  730, 4005,  ..., 6063,  239, 1950]),\n",
       " tensor([2747, 8214,  577,  ..., 8903, 3311, 5138])]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(loader3)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "9f8b8471",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([31356,  9109, 16324, 38036, 51295, 42638, 24647, 21514, 23996, 35992,\n",
      "        43159, 39517, 58448, 35275, 58960, 39325, 27886,  3564, 44998, 50012,\n",
      "        12426, 41379,  9438, 14740, 25152, 22139, 54755, 17384, 41320, 53979,\n",
      "        56378, 13041, 45405, 61555, 39860, 36136, 45683, 12759, 61423, 29373,\n",
      "        51373, 34731,   365, 25318, 40412, 12368, 47921, 17466, 24225, 24152,\n",
      "        37262, 55110, 10010,  1317, 37671, 31190, 25261,  3058, 49664,  6286,\n",
      "        36962, 60626, 34628, 29122, 20798, 14673, 18775, 20911, 13969, 18673,\n",
      "        25793, 24120, 52356, 26266,  6204, 49686, 46993, 50397, 42158, 11579,\n",
      "        29126, 39047, 13185,  2919, 12513, 24487, 57561, 41064, 41894, 23470,\n",
      "        11402, 12680, 52914, 43357, 62412, 28219,  9700, 56366, 51209, 29190,\n",
      "        46756, 34359, 58986, 31142, 34065, 25500, 14597, 43172, 53508, 47810,\n",
      "        12663,   625, 46909, 51644,  3128,  5659, 38029, 38846, 46762, 14519,\n",
      "         5691,  8513, 40987, 10202, 58875], dtype=torch.int32) tensor([16,  9, 10, 15, 16, 10, 15, 15, 16, 16, 16, 10, 16, 11, 16, 13, 10,  0,\n",
      "        10, 16,  9, 16,  9,  9, 16, 15, 16, 12, 10, 16, 15,  9, 10, 16, 16, 16,\n",
      "        15,  9, 13, 16, 15, 15,  0, 15, 16,  9, 15, 13, 16, 16, 10, 15,  9,  0,\n",
      "        15, 16, 10,  0, 10,  9, 16, 10, 10, 16, 16,  9, 16, 16,  9, 16, 16, 10,\n",
      "        16, 10,  9, 16, 16, 16, 10,  9, 16, 11,  9,  0,  9, 16, 15, 15, 16, 15,\n",
      "         9,  1, 14, 16, 15, 16,  9, 10, 13, 10, 16, 10, 15, 10, 16, 16,  9, 15,\n",
      "        16, 10,  9,  0, 16, 16,  0,  0, 10, 15, 15,  7,  0,  9, 16,  9, 10]) tensor([10474,  4346, 15279, 13546, 16230,  7741,  8490, 13063, 15402,  7126,\n",
      "        15766, 13381, 11090,  9386,  9250, 13353,  6793,  8683, 13127, 14018,\n",
      "         2085,  9819,   208,  1953, 15773, 15894, 13692,  7662, 11366, 13798,\n",
      "         6979,  5838,  7341,  6908, 13340,  7897, 15344,  5148, 13458, 14894,\n",
      "        14864, 15517, 10194, 11487, 11794,  1673,  7307, 15080, 12360, 10137,\n",
      "         9277,  6637,   163, 14562,  7907, 13812, 11272,  6070, 10539,  4372,\n",
      "         7677,  6095, 14928, 14289,  9951,  1112, 16075, 14939,   654, 52296,\n",
      "         6927, 13727,  7793,  9923,  1096,  9679, 11778,  8271, 15626,   543,\n",
      "        13812, 11293,  5211,  6289,  3786, 15069, 12691, 15061, 12434,  9009,\n",
      "         3398, 59460, 11206,  9620, 15760, 10747,  1871, 12209, 14386, 10012,\n",
      "        12409, 13876, 11610, 13692,  6735, 10747,  5038, 11765, 13490,  8176,\n",
      "         1516,  7730, 15788, 12359, 13105, 15941,  8938,  6190,  8092, 57439,\n",
      "        13948,  1633, 15763,  1557, 13859], dtype=torch.int32) tensor([57172, 36031, 35181, 63288, 12722,   167, 57897, 49808, 32740, 26422,\n",
      "        60951, 61593,  9517, 63153,  7892, 36976,  8078, 40203, 44239,  2845,\n",
      "        51108, 27551, 45233,  6824, 62248, 54985,  9735,  6605, 46979,  3716,\n",
      "        23347, 59957, 31392, 55173, 55826,   361,  2193, 45557,  9535, 34499,\n",
      "        62812, 32578, 15289, 31869,  7912, 60443, 53086, 28199, 45853,  2906,\n",
      "        59449, 29886, 26198, 16701, 37211, 17655, 51950, 13144, 40702, 16134,\n",
      "         2071, 20775, 24666, 48663, 30050, 31556, 12661, 53251, 53815, 53954,\n",
      "        35056, 14826,  5478,    12, 40356, 24934, 39529, 49578, 48273, 60333,\n",
      "        20079, 57838, 21200, 10082,  7149, 40928, 26727, 29610, 21075,  3144,\n",
      "        40794, 38656, 17398,   295,  2175, 32528, 51989, 61922, 59426,  4274,\n",
      "        38227,  9762, 15371, 47325, 36154, 15283, 29514, 30368, 27704,    95,\n",
      "        40681, 46983, 48085, 23604, 51163, 34554, 57514, 13379, 27306, 33320,\n",
      "        44725, 10565,  2957, 50405, 30761])\n"
     ]
    }
   ],
   "source": [
    "def fn(c1,c2,c3, c4):\n",
    "    print(c1, c2, c3, c4)\n",
    "fn(*batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e11c6212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 100/100 [00:00<00:00, 16888.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.799531936645508e-05   0.0002919245984303588\n",
      "[-0.000234,0.000350]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "core_count = multiprocessing.cpu_count()\n",
    "loader = DataLoader(ds, batch_size=ds.batch_size, \n",
    "                    shuffle=True,  \n",
    "                    num_workers=core_count,\n",
    "                   persistent_workers=True)\n",
    "loader_ds = cycle(loader)\n",
    "\n",
    "loader1 = cycle(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=0))\n",
    "loader2 = cycle(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=4))\n",
    "loader3 = cycle(DataLoader(iter_ds,batch_size=ds.batch_size, num_workers=32))\n",
    "\n",
    "loader_iter = loader_ds\n",
    "\n",
    "l = []\n",
    "for _ in tqdm(range(100)):\n",
    "    start = time()\n",
    "    batch = next(loader_ds)\n",
    "    end = time()\n",
    "    #print(end-start)\n",
    "    l.append(end-start)\n",
    "print(np.mean(l), ' ', np.std(l))\n",
    "print(\"[%f,%f]\" %(np.mean(l)-np.std(l),np.mean(l)+ np.std(l))  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41142fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[23566, 48123]\n",
      "[n_train, n_test]=[1289003, 423635]\n",
      "[n_entities, n_relations, n_triples]=[106389, 9, 464567]\n",
      "[batch_size, batch_size_kg]=[1024, 369]\n"
     ]
    }
   ],
   "source": [
    "from utility.load_data import RecomDataset\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "ds = RecomDataset(args=args, path=args.data_path + args.dataset)\n",
    "core_count = multiprocessing.cpu_count()\n",
    "loader = DataLoader(ds, batch_size=ds.batch_size, \n",
    "                    shuffle=True,  \n",
    "                    num_workers=core_count)\n",
    "\n",
    "\n",
    "loader_iter = iter(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "deeea7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 30.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03316580533981323   0.15816616950541\n",
      "[-0.125000,0.191332]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "l = []\n",
    "for _ in tqdm(range(100)):\n",
    "    start = time()\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    \n",
    "    \n",
    "    end = time()\n",
    "    l.append(end-start)\n",
    "print(np.mean(l), ' ', np.std(l))\n",
    "print(\"[%f,%f]\" %(np.mean(l)-np.std(l),np.mean(l)+ np.std(l))  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0276a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "l = []\n",
    "for _ in tqdm(range(100)):\n",
    "    start = time()\n",
    "    try:\n",
    "        batch = next(loader_iter)\n",
    "    except:\n",
    "        loader_iter = iter(loader)\n",
    "        batch = next(loader_iter)\n",
    "    \n",
    "    \n",
    "    end = time()\n",
    "    l.append(end-start)\n",
    "print(np.mean(l), ' ', np.std(l))\n",
    "print(\"[%f,%f]\" %(np.mean(l)-np.std(l),np.mean(l)+ np.std(l))  )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422dc3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b9076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "aecd75f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1128, 1645, 2200,  ..., 3229, 4761, 3308]),\n",
       " tensor([ 596, 7074, 2122,  ..., 3759, 8851, 5591]),\n",
       " tensor([7503, 4644, 1610,  ..., 5818, 6779, 2261])]"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "1e3e24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Dec 18, 2018\n",
    "Tensorflow Implementation of Knowledge Graph Attention Network (KGAT) model in:\n",
    "Wang Xiang et al. KGAT: Knowledge Graph Attention Network for Recommendation. In KDD 2019.\n",
    "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
    "'''\n",
    "import numpy as np\n",
    "from utility.load_data import Data\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "import random as rd\n",
    "import collections\n",
    "\n",
    "class KGATDataset(RecomDataset):\n",
    "    def __init__(self, args, path):\n",
    "        super().__init__(args, path)\n",
    "\n",
    "        # generate the sparse adjacency matrices for user-item interaction & relational kg data.\n",
    "        self.adj_list, self.adj_r_list = self._get_relational_adj_list()\n",
    "\n",
    "        # generate the sparse laplacian matrices.\n",
    "        self.lap_list = self._get_relational_lap_list()\n",
    "\n",
    "        # generate the triples dictionary, key is 'head', value is '(tail, relation)'.\n",
    "        self.all_kg_dict = self._get_all_kg_dict()\n",
    "        self.exist_heads = list(self.all_kg_dict.keys())\n",
    "        self.N_exist_heads = len(self.exist_heads)\n",
    "        \n",
    "        self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()\n",
    "        \n",
    "\n",
    "    def _get_relational_adj_list(self):\n",
    "        t1 = time()\n",
    "        adj_mat_list = []\n",
    "        adj_r_list = []\n",
    "\n",
    "        def _np_mat2sp_adj(np_mat, row_pre, col_pre):\n",
    "            n_all = self.n_users + self.n_entities\n",
    "            # single-direction\n",
    "            a_rows = np_mat[:, 0] + row_pre\n",
    "            a_cols = np_mat[:, 1] + col_pre\n",
    "            a_vals = [1.] * len(a_rows)\n",
    "\n",
    "            b_rows = a_cols\n",
    "            b_cols = a_rows\n",
    "            b_vals = [1.] * len(b_rows)\n",
    "\n",
    "            a_adj = sp.coo_matrix((a_vals, (a_rows, a_cols)), shape=(n_all, n_all))\n",
    "            b_adj = sp.coo_matrix((b_vals, (b_rows, b_cols)), shape=(n_all, n_all))\n",
    "\n",
    "            return a_adj, b_adj\n",
    "\n",
    "        R, R_inv = _np_mat2sp_adj(self.train_data, row_pre=0, col_pre=self.n_users)\n",
    "        adj_mat_list.append(R)\n",
    "        adj_r_list.append(0)\n",
    "\n",
    "        adj_mat_list.append(R_inv)\n",
    "        adj_r_list.append(self.n_relations + 1)\n",
    "        print('\\tconvert ratings into adj mat done.')\n",
    "\n",
    "        for r_id in self.relation_dict.keys():\n",
    "            K, K_inv = _np_mat2sp_adj(np.array(self.relation_dict[r_id]), row_pre=self.n_users, col_pre=self.n_users)\n",
    "            adj_mat_list.append(K)\n",
    "            adj_r_list.append(r_id + 1)\n",
    "\n",
    "            adj_mat_list.append(K_inv)\n",
    "            adj_r_list.append(r_id + 2 + self.n_relations)\n",
    "        print('\\tconvert %d relational triples into adj mat done. @%.4fs' %(len(adj_mat_list), time()-t1))\n",
    "\n",
    "        self.n_relations = len(adj_r_list)\n",
    "        # print('\\tadj relation list is', adj_r_list)\n",
    "\n",
    "        return adj_mat_list, adj_r_list\n",
    "\n",
    "    def _get_relational_lap_list(self):\n",
    "        def _bi_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def _si_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.args.adj_type == 'bi':\n",
    "            lap_list = [_bi_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate bi-normalized adjacency matrix.')\n",
    "        else:\n",
    "            lap_list = [_si_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate si-normalized adjacency matrix.')\n",
    "        return lap_list\n",
    "\n",
    "    def _get_all_kg_dict(self):\n",
    "        all_kg_dict = collections.defaultdict(list)\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "\n",
    "            rows = lap.row\n",
    "            cols = lap.col\n",
    "\n",
    "            for i_id in range(len(rows)):\n",
    "                head = rows[i_id]\n",
    "                tail = cols[i_id]\n",
    "                relation = self.adj_r_list[l_id]\n",
    "\n",
    "                all_kg_dict[head].append((tail, relation))\n",
    "        return all_kg_dict\n",
    "\n",
    "    def _get_all_kg_data(self):\n",
    "        def _reorder_list(org_list, order):\n",
    "            new_list = np.array(org_list)\n",
    "            new_list = new_list[order]\n",
    "            return new_list\n",
    "\n",
    "        all_h_list, all_t_list, all_r_list = [], [], []\n",
    "        all_v_list = []\n",
    "\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "            all_h_list += list(lap.row)\n",
    "            all_t_list += list(lap.col)\n",
    "            all_v_list += list(lap.data)\n",
    "            all_r_list += [self.adj_r_list[l_id]] * len(lap.row)\n",
    "\n",
    "        assert len(all_h_list) == sum([len(lap.data) for lap in self.lap_list])\n",
    "\n",
    "        # resort the all_h/t/r/v_list,\n",
    "        # ... since tensorflow.sparse.softmax requires indices sorted in the canonical lexicographic order\n",
    "        print('\\treordering indices...')\n",
    "        org_h_dict = dict()\n",
    "\n",
    "        for idx, h in enumerate(all_h_list):\n",
    "            if h not in org_h_dict.keys():\n",
    "                org_h_dict[h] = [[],[],[]]\n",
    "\n",
    "            org_h_dict[h][0].append(all_t_list[idx])\n",
    "            org_h_dict[h][1].append(all_r_list[idx])\n",
    "            org_h_dict[h][2].append(all_v_list[idx])\n",
    "        print('\\treorganize all kg data done.')\n",
    "\n",
    "        sorted_h_dict = dict()\n",
    "        for h in org_h_dict.keys():\n",
    "            org_t_list, org_r_list, org_v_list = org_h_dict[h]\n",
    "            sort_t_list = np.array(org_t_list)\n",
    "            sort_order = np.argsort(sort_t_list)\n",
    "\n",
    "            sort_t_list = _reorder_list(org_t_list, sort_order)\n",
    "            sort_r_list = _reorder_list(org_r_list, sort_order)\n",
    "            sort_v_list = _reorder_list(org_v_list, sort_order)\n",
    "\n",
    "            sorted_h_dict[h] = [sort_t_list, sort_r_list, sort_v_list]\n",
    "        print('\\tsort meta-data done.')\n",
    "\n",
    "        od = collections.OrderedDict(sorted(sorted_h_dict.items()))\n",
    "        new_h_list, new_t_list, new_r_list, new_v_list = [], [], [], []\n",
    "\n",
    "        for h, vals in od.items():\n",
    "            new_h_list += [h] * len(vals[0])\n",
    "            new_t_list += list(vals[0])\n",
    "            new_r_list += list(vals[1])\n",
    "            new_v_list += list(vals[2])\n",
    "\n",
    "\n",
    "        assert sum(new_h_list) == sum(all_h_list)\n",
    "        assert sum(new_t_list) == sum(all_t_list)\n",
    "        assert sum(new_r_list) == sum(all_r_list)\n",
    "        # try:\n",
    "        #     assert sum(new_v_list) == sum(all_v_list)\n",
    "        # except Exception:\n",
    "        #     print(sum(new_v_list), '\\n')\n",
    "        #     print(sum(all_v_list), '\\n')\n",
    "        print('\\tsort all data done.')\n",
    "\n",
    "\n",
    "        return new_h_list, new_r_list, new_t_list, new_v_list\n",
    "\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        # number of existing users after the preprocessing described in the paper, \n",
    "        # determines the length of the training dataset, for which a positive an negative are extracted\n",
    "        return self.N_exist_heads \n",
    "    \n",
    "    ##_generate_train_A_batch\n",
    "    def __getitem__(self, idx):      \n",
    "        \n",
    "        '''\n",
    "        exist_heads = self.all_kg_dict.keys()\n",
    "\n",
    "        if self.batch_size_kg <= len(exist_heads):\n",
    "            heads = rd.sample(exist_heads, self.batch_size_kg)\n",
    "        else:\n",
    "            heads = [rd.choice(exist_heads) for _ in range(self.batch_size_kg)]\n",
    "        '''\n",
    "        \n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.all_kg_dict[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_users + self.n_entities, size=1)[0]\n",
    "                if (t, r) not in self.all_kg_dict[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "        '''\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "        \n",
    "        '''\n",
    "        h = self.exist_heads[idx]\n",
    "        pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "        neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "\n",
    "        if len(pos_rs) == 1:\n",
    "            pos_rs = pos_rs[0]  \n",
    "        if len(pos_ts) == 1:\n",
    "            pos_ts = pos_ts[0]  \n",
    "        if len(neg_ts) == 1:\n",
    "            neg_ts = neg_ts[0]              \n",
    "        \n",
    "        return h, pos_rs, pos_ts, neg_ts\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def as_train_feed_dict(args, model, users, pos_items, neg_items):\n",
    "    batch_data = {}\n",
    "    batch_data['users'] = users\n",
    "    batch_data['pos_items'] = pos_items\n",
    "    batch_data['neg_items'] = neg_items\n",
    "    feed_dict = {\n",
    "        model.users: batch_data['users'],\n",
    "        model.pos_items: batch_data['pos_items'],\n",
    "        model.neg_items: batch_data['neg_items'],\n",
    "\n",
    "        model.mess_dropout: eval(args.mess_dropout),\n",
    "        model.node_dropout: eval(args.node_dropout),\n",
    "    }\n",
    "\n",
    "    return feed_dict    \n",
    "\n",
    "def as_train_A_feed_dict(model, heads, relations, pos_tails, neg_tails ):\n",
    "\n",
    "    batch_data = {}\n",
    "\n",
    "    batch_data['heads'] = heads\n",
    "    batch_data['relations'] = relations\n",
    "    batch_data['pos_tails'] = pos_tails\n",
    "    batch_data['neg_tails'] = neg_tails\n",
    "    \n",
    "    feed_dict = {\n",
    "        model.h: batch_data['heads'],\n",
    "        model.r: batch_data['relations'],\n",
    "        model.pos_t: batch_data['pos_tails'],\n",
    "        model.neg_t: batch_data['neg_tails'],\n",
    "\n",
    "    }\n",
    "\n",
    "    return feed_dict\n",
    "def as_test_feed_dict(args, model, user_batch, item_batch, drop_flag=True):\n",
    "\n",
    "    feed_dict ={\n",
    "        model.users: user_batch,\n",
    "        model.pos_items: item_batch,\n",
    "        model.mess_dropout: [0.] * len(eval(args.layer_size)),\n",
    "        model.node_dropout: [0.] * len(eval(args.layer_size)),\n",
    "\n",
    "    }\n",
    "\n",
    "    return feed_dict  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "ea35ae04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rd.randint(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6fb5a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[5941, 10303]\n",
      "[n_train, n_test]=[738345, 217212]\n",
      "[n_entities, n_relations, n_triples]=[57541, 8, 90778]\n",
      "[batch_size, batch_size_kg]=[1024, 125]\n",
      "\tconvert ratings into adj mat done.\n",
      "\tconvert 18 relational triples into adj mat done. @0.1288s\n",
      "\tgenerate si-normalized adjacency matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_194571/2707159442.py:89: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treordering indices...\n",
      "\treorganize all kg data done.\n",
      "\tsort meta-data done.\n",
      "\tsort all data done.\n"
     ]
    }
   ],
   "source": [
    "kgat_ds = KGATDataset(args=args, path=args.data_path + args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "066b5f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 7983, 44570)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "super(kgat_ds.__class__, kgat_ds).__getitem__(0)\n",
    "    \n",
    "kgat_ds.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "bbec580c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_194571/2528349239.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkgat_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkgat_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'super' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "super(kgat_ds.__class__, kgat_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "7a0d52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 910.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010899925231933594   0.0021670084959428157\n",
      "[-0.001077,0.003257]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import cycle\n",
    "\n",
    "core_count = multiprocessing.cpu_count()\n",
    "kgat_loader = DataLoader(kgat_ds, batch_size=kgat_ds.batch_size_kg, \n",
    "                    shuffle=True,  \n",
    "                    num_workers=32,\n",
    "                   #persistent_workers=True\n",
    "                        )\n",
    "loader_kgat_ds = cycle(kgat_loader)\n",
    "\n",
    "\n",
    "l = []\n",
    "for _ in tqdm(range(100)):\n",
    "    start = time()\n",
    "    batch = next(loader_kgat_ds)\n",
    "    end = time()\n",
    "    #print(end-start)\n",
    "    l.append(end-start)\n",
    "print(np.mean(l), ' ', np.std(l))\n",
    "print(\"[%f,%f]\" %(np.mean(l)-np.std(l),np.mean(l)+ np.std(l))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2622b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([40758, 17173, 47286,   481, 17038, 57327, 17672, 43096, 26978, 40043,\n",
       "         36673,  1865, 20711, 50442, 60304, 40633,  6038, 26757,  3305, 15256,\n",
       "         23322,  7956, 38352, 61538, 20197, 50968, 19723, 41352, 35853, 39749,\n",
       "         10007, 11522, 62552, 62707, 18098, 17808,  3419,  7526, 18766, 12641,\n",
       "         60592, 10366,  7077, 39003, 52221, 10915, 36686, 53820,  4621,  1015,\n",
       "         38155, 42014, 14352, 12362,  4570, 27881, 33962, 25807, 41817, 26405,\n",
       "         12418, 44195, 59566, 46122, 54609, 11626, 60738, 35710, 16029,  5230,\n",
       "         47140, 36737, 21907, 19452, 33306, 10240, 18248, 38662,  6869, 61516,\n",
       "         11269, 56515,  3240, 62885, 39318,  4977, 25798, 61419, 58690, 23221,\n",
       "         27811, 39259, 36134, 43356, 22826, 38197, 43617, 62967, 60947,  5163,\n",
       "         29648, 26834,  8828, 16490,  4789, 10630, 61034, 46702, 56493,  4060,\n",
       "         32725, 10199, 15207, 24317, 34688, 30209, 46794,  4140, 22091, 57716,\n",
       "         49542,  2797, 36824, 24639, 16292], dtype=torch.int32),\n",
       " tensor([11, 10, 16,  0, 16, 15, 10, 10, 15, 10, 10,  0, 16, 16, 16, 16,  9, 16,\n",
       "          0,  9, 10,  9, 10, 10, 14, 12, 15, 16, 16, 16,  9,  9, 10, 16, 16, 16,\n",
       "          0,  9, 15,  9, 15,  9,  9, 10, 15,  9, 15, 16,  0,  0, 10, 16,  9,  1,\n",
       "          0, 15, 10, 16, 15, 16,  9, 10, 16, 10, 10,  9, 14, 16,  9,  0, 15, 15,\n",
       "         12, 16, 16,  9, 16, 10,  9, 16,  9, 10,  0, 16, 10,  0, 16, 10, 16, 13,\n",
       "         10, 15, 15, 16, 10, 16, 10, 15, 16,  0, 15, 16,  9, 15,  0,  9, 10, 15,\n",
       "         16,  0, 16,  9,  2, 15, 10, 16, 10,  0, 15, 16, 15,  0, 15, 16, 10]),\n",
       " tensor([ 6537, 10281,  6668,  8709,  7170,  6336, 12635,  8927,  8547, 11058,\n",
       "         11395, 14598, 15758, 14415, 12260,  8123,  5656, 12179, 11812,  5817,\n",
       "          7900,  1694, 10560,  9099,  7924, 14078,  9227,  7501,  7862, 15164,\n",
       "          5148,   810,  8088,  6697,  6211, 10853, 10732,  1775, 15184,  2719,\n",
       "          9818,  5766,  5797,  7587, 15663,  3966,  6436,  7295, 15311, 14082,\n",
       "         12423,  6766,  1062, 54860,  8354, 10484, 12004,  9306, 14379,  7638,\n",
       "          5911, 15675,  6811, 14981, 54600,  4644,  6826, 14025,   141, 14707,\n",
       "         13979, 52296,  6079,  6947, 13574,  5274, 14056, 11623,  5033, 14410,\n",
       "          3103, 13706, 15775,  6280,  7350,  7703,  9868,  6025, 12808,  7986,\n",
       "         13828, 13363,  8598, 10447,  8465, 15555, 10069, 12348, 10261, 15795,\n",
       "          6827,  9973,  5757, 12490,  8143,  5605, 10560, 10609, 13596,  9212,\n",
       "          6647,  3450, 43616, 13781, 12097, 10006,  9868, 12216,  7790,  6504,\n",
       "          8681, 15249, 11291,  6647,  8999], dtype=torch.int32),\n",
       " tensor([61588, 13786,  3911, 41353, 10972, 56077, 45168, 13201,  7187,  7857,\n",
       "         11238, 43388, 54903,  4195, 24830, 22124, 43284, 24167,  1323,  1218,\n",
       "         10741, 56983, 56619, 45074, 56570, 23314, 23693, 51519, 39918, 40026,\n",
       "         32520, 15077, 60201,  6886, 28240, 58899, 35948, 37918, 26937, 53551,\n",
       "         61664, 47654, 55086, 42958, 40325,  6409, 55676, 27224, 31728, 45593,\n",
       "         19222, 53504, 19785,  7248, 27624, 13600, 38959, 22554, 11355, 10739,\n",
       "         14143, 34441, 58365, 62968, 21863, 17361, 13621, 23205, 24542, 52500,\n",
       "         60504, 10101, 49800, 62603,    77, 38569, 17080, 42652, 63450, 17874,\n",
       "          7933, 33219, 53597, 17325, 13017, 50779, 60556, 11911, 56548, 30104,\n",
       "         44970, 38470, 37454, 44828, 45531, 16166, 15006, 61326, 17014, 26522,\n",
       "         36616, 30158, 17279, 20600, 20776, 50235, 39778,  4377, 59373,  6607,\n",
       "         34350, 47108, 53518, 20506, 26087, 45308, 59768, 29691, 52892, 26946,\n",
       "         37904, 25965, 38835, 51120, 26349])]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(loader_kgat_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9bdfdb0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, [0], [9516], [22507])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ds = BPRMF_loader(args=args, path=args.data_path + args.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3651c6",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "56e1cc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 16,  9,  9, 15,  1,  0,  0, 10,  9, 16,  9, 10, 11, 16, 16, 10, 16,\n",
       "        10,  0, 16,  0, 11,  9, 16, 10,  9,  9,  0, 16, 16,  0,  9, 15, 16, 15,\n",
       "        16, 15, 10, 16, 11,  9, 10, 16, 16,  9, 10, 16, 15, 16, 15,  9, 16, 15,\n",
       "        15, 16, 14, 16,  0, 16,  0, 15, 14, 16, 11, 15, 16,  9,  9, 10, 16, 15,\n",
       "        15, 16, 14, 10, 10,  9,  0, 16, 10, 15,  0, 15, 16, 10, 16, 10, 16, 16,\n",
       "        16, 16,  9,  9,  0, 10, 16,  9, 15, 11, 10, 16,  9, 10, 16, 16, 16, 11,\n",
       "        16,  9, 10, 10, 10,  9, 10, 10,  0, 11, 16, 16, 10, 16,  9, 10, 10])"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b783937d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kgat_ds.batch_size_kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab30e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51c7a568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utility.load_data import Data\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "import random as rd\n",
    "import collections\n",
    "\n",
    "class KGAT_loader(Data):\n",
    "    def __init__(self, args, path):\n",
    "        super().__init__(args, path)\n",
    "\n",
    "        # generate the sparse adjacency matrices for user-item interaction & relational kg data.\n",
    "        self.adj_list, self.adj_r_list = self._get_relational_adj_list()\n",
    "\n",
    "        # generate the sparse laplacian matrices.\n",
    "        self.lap_list = self._get_relational_lap_list()\n",
    "\n",
    "        # generate the triples dictionary, key is 'head', value is '(tail, relation)'.\n",
    "        self.all_kg_dict = self._get_all_kg_dict()\n",
    "\n",
    "        self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()\n",
    "\n",
    "\n",
    "    def _get_relational_adj_list(self):\n",
    "        t1 = time()\n",
    "        adj_mat_list = []\n",
    "        adj_r_list = []\n",
    "\n",
    "        def _np_mat2sp_adj(np_mat, row_pre, col_pre):\n",
    "            n_all = self.n_users + self.n_entities\n",
    "            # single-direction\n",
    "            a_rows = np_mat[:, 0] + row_pre\n",
    "            a_cols = np_mat[:, 1] + col_pre\n",
    "            a_vals = [1.] * len(a_rows)\n",
    "\n",
    "            b_rows = a_cols\n",
    "            b_cols = a_rows\n",
    "            b_vals = [1.] * len(b_rows)\n",
    "\n",
    "            a_adj = sp.coo_matrix((a_vals, (a_rows, a_cols)), shape=(n_all, n_all))\n",
    "            b_adj = sp.coo_matrix((b_vals, (b_rows, b_cols)), shape=(n_all, n_all))\n",
    "\n",
    "            return a_adj, b_adj\n",
    "\n",
    "        R, R_inv = _np_mat2sp_adj(self.train_data, row_pre=0, col_pre=self.n_users)\n",
    "        adj_mat_list.append(R)\n",
    "        adj_r_list.append(0)\n",
    "\n",
    "        adj_mat_list.append(R_inv)\n",
    "        adj_r_list.append(self.n_relations + 1)\n",
    "        print('\\tconvert ratings into adj mat done.')\n",
    "\n",
    "        for r_id in self.relation_dict.keys():\n",
    "            K, K_inv = _np_mat2sp_adj(np.array(self.relation_dict[r_id]), row_pre=self.n_users, col_pre=self.n_users)\n",
    "            adj_mat_list.append(K)\n",
    "            adj_r_list.append(r_id + 1)\n",
    "\n",
    "            adj_mat_list.append(K_inv)\n",
    "            adj_r_list.append(r_id + 2 + self.n_relations)\n",
    "        print('\\tconvert %d relational triples into adj mat done. @%.4fs' %(len(adj_mat_list), time()-t1))\n",
    "\n",
    "        self.n_relations = len(adj_r_list)\n",
    "        # print('\\tadj relation list is', adj_r_list)\n",
    "\n",
    "        return adj_mat_list, adj_r_list\n",
    "\n",
    "    def _get_relational_lap_list(self):\n",
    "        def _bi_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def _si_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.args.adj_type == 'bi':\n",
    "            lap_list = [_bi_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate bi-normalized adjacency matrix.')\n",
    "        else:\n",
    "            lap_list = [_si_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate si-normalized adjacency matrix.')\n",
    "        return lap_list\n",
    "\n",
    "    def _get_all_kg_dict(self):\n",
    "        all_kg_dict = collections.defaultdict(list)\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "\n",
    "            rows = lap.row\n",
    "            cols = lap.col\n",
    "\n",
    "            for i_id in range(len(rows)):\n",
    "                head = rows[i_id]\n",
    "                tail = cols[i_id]\n",
    "                relation = self.adj_r_list[l_id]\n",
    "\n",
    "                all_kg_dict[head].append((tail, relation))\n",
    "        return all_kg_dict\n",
    "\n",
    "    def _get_all_kg_data(self):\n",
    "        def _reorder_list(org_list, order):\n",
    "            new_list = np.array(org_list)\n",
    "            new_list = new_list[order]\n",
    "            return new_list\n",
    "\n",
    "        all_h_list, all_t_list, all_r_list = [], [], []\n",
    "        all_v_list = []\n",
    "\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "            all_h_list += list(lap.row)\n",
    "            all_t_list += list(lap.col)\n",
    "            all_v_list += list(lap.data)\n",
    "            all_r_list += [self.adj_r_list[l_id]] * len(lap.row)\n",
    "\n",
    "        assert len(all_h_list) == sum([len(lap.data) for lap in self.lap_list])\n",
    "\n",
    "        # resort the all_h/t/r/v_list,\n",
    "        # ... since tensorflow.sparse.softmax requires indices sorted in the canonical lexicographic order\n",
    "        print('\\treordering indices...')\n",
    "        org_h_dict = dict()\n",
    "\n",
    "        for idx, h in enumerate(all_h_list):\n",
    "            if h not in org_h_dict.keys():\n",
    "                org_h_dict[h] = [[],[],[]]\n",
    "\n",
    "            org_h_dict[h][0].append(all_t_list[idx])\n",
    "            org_h_dict[h][1].append(all_r_list[idx])\n",
    "            org_h_dict[h][2].append(all_v_list[idx])\n",
    "        print('\\treorganize all kg data done.')\n",
    "\n",
    "        sorted_h_dict = dict()\n",
    "        for h in org_h_dict.keys():\n",
    "            org_t_list, org_r_list, org_v_list = org_h_dict[h]\n",
    "            sort_t_list = np.array(org_t_list)\n",
    "            sort_order = np.argsort(sort_t_list)\n",
    "\n",
    "            sort_t_list = _reorder_list(org_t_list, sort_order)\n",
    "            sort_r_list = _reorder_list(org_r_list, sort_order)\n",
    "            sort_v_list = _reorder_list(org_v_list, sort_order)\n",
    "\n",
    "            sorted_h_dict[h] = [sort_t_list, sort_r_list, sort_v_list]\n",
    "        print('\\tsort meta-data done.')\n",
    "\n",
    "        od = collections.OrderedDict(sorted(sorted_h_dict.items()))\n",
    "        new_h_list, new_t_list, new_r_list, new_v_list = [], [], [], []\n",
    "\n",
    "        for h, vals in od.items():\n",
    "            new_h_list += [h] * len(vals[0])\n",
    "            new_t_list += list(vals[0])\n",
    "            new_r_list += list(vals[1])\n",
    "            new_v_list += list(vals[2])\n",
    "\n",
    "\n",
    "        assert sum(new_h_list) == sum(all_h_list)\n",
    "        assert sum(new_t_list) == sum(all_t_list)\n",
    "        assert sum(new_r_list) == sum(all_r_list)\n",
    "        # try:\n",
    "        #     assert sum(new_v_list) == sum(all_v_list)\n",
    "        # except Exception:\n",
    "        #     print(sum(new_v_list), '\\n')\n",
    "        #     print(sum(all_v_list), '\\n')\n",
    "        print('\\tsort all data done.')\n",
    "\n",
    "\n",
    "        return new_h_list, new_r_list, new_t_list, new_v_list\n",
    "\n",
    "    def _generate_train_A_batch(self):\n",
    "        exist_heads = self.all_kg_dict.keys()\n",
    "\n",
    "        if self.batch_size_kg <= len(exist_heads):\n",
    "            heads = rd.sample(exist_heads, self.batch_size_kg)\n",
    "        else:\n",
    "            heads = [rd.choice(exist_heads) for _ in range(self.batch_size_kg)]\n",
    "\n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.all_kg_dict[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_users + self.n_entities, size=1)[0]\n",
    "                if (t, r) not in self.all_kg_dict[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "\n",
    "        return heads, pos_r_batch, pos_t_batch, neg_t_batch\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        users, pos_items, neg_items = self._generate_train_cf_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "        batch_data['users'] = users\n",
    "        batch_data['pos_items'] = pos_items\n",
    "        batch_data['neg_items'] = neg_items\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.users: batch_data['users'],\n",
    "            model.pos_items: batch_data['pos_items'],\n",
    "            model.neg_items: batch_data['neg_items'],\n",
    "\n",
    "            model.mess_dropout: eval(self.args.mess_dropout),\n",
    "            model.node_dropout: eval(self.args.node_dropout),\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def generate_train_A_batch(self):\n",
    "        heads, relations, pos_tails, neg_tails = self._generate_train_A_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "\n",
    "        batch_data['heads'] = heads\n",
    "        batch_data['relations'] = relations\n",
    "        batch_data['pos_tails'] = pos_tails\n",
    "        batch_data['neg_tails'] = neg_tails\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_A_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.h: batch_data['heads'],\n",
    "            model.r: batch_data['relations'],\n",
    "            model.pos_t: batch_data['pos_tails'],\n",
    "            model.neg_t: batch_data['neg_tails'],\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "\n",
    "    def generate_test_feed_dict(self, model, user_batch, item_batch, drop_flag=True):\n",
    "\n",
    "        feed_dict ={\n",
    "            model.users: user_batch,\n",
    "            model.pos_items: item_batch,\n",
    "            model.mess_dropout: [0.] * len(eval(self.args.layer_size)),\n",
    "            model.node_dropout: [0.] * len(eval(self.args.layer_size)),\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c6acf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[23566, 48123]\n",
      "[n_train, n_test]=[1289003, 423635]\n",
      "[n_entities, n_relations, n_triples]=[106389, 9, 464567]\n",
      "[batch_size, batch_size_kg]=[1024, 369]\n",
      "\tconvert ratings into adj mat done.\n",
      "\tconvert 20 relational triples into adj mat done. @0.5037s\n",
      "\tgenerate si-normalized adjacency matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_633108/104162359.py:81: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treordering indices...\n",
      "\treorganize all kg data done.\n",
      "\tsort meta-data done.\n",
      "\tsort all data done.\n"
     ]
    }
   ],
   "source": [
    "kgat_old = KGAT_loader(args=args, path=args.data_path + args.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56aa091e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 89.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011104698181152345   0.0030025478494529077\n",
      "[0.008102,0.014107]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l = []\n",
    "for _ in tqdm(range(100)):\n",
    "    start = time()\n",
    "    #batch = kgat_old._generate_train_A_batch()#kgat_old.generate_train_A_batch()\n",
    "    batch = kgat_old.generate_train_A_batch()\n",
    "    end = time()\n",
    "    #print(end-start)\n",
    "    l.append(end-start)\n",
    "print(np.mean(l), ' ', np.std(l))\n",
    "print(\"[%f,%f]\" %(np.mean(l)-np.std(l),np.mean(l)+ np.std(l))  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "27f94314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26360,\n",
       " 6585,\n",
       " 51837,\n",
       " 31212,\n",
       " 9938,\n",
       " 48410,\n",
       " 50537,\n",
       " 21284,\n",
       " 21270,\n",
       " 15356,\n",
       " 39238,\n",
       " 42931,\n",
       " 52159,\n",
       " 14574,\n",
       " 41337,\n",
       " 15111,\n",
       " 29847,\n",
       " 36652,\n",
       " 11704,\n",
       " 1304,\n",
       " 46486,\n",
       " 8695,\n",
       " 39658,\n",
       " 18680,\n",
       " 1741,\n",
       " 52558,\n",
       " 9390,\n",
       " 41825,\n",
       " 1733,\n",
       " 36358,\n",
       " 43469,\n",
       " 39326,\n",
       " 58626,\n",
       " 50858,\n",
       " 38027,\n",
       " 4628,\n",
       " 21042,\n",
       " 37223,\n",
       " 45190,\n",
       " 34365,\n",
       " 54834,\n",
       " 5690,\n",
       " 10640,\n",
       " 18666,\n",
       " 40475,\n",
       " 35927,\n",
       " 49294,\n",
       " 24762,\n",
       " 16509,\n",
       " 1560,\n",
       " 53058,\n",
       " 21849,\n",
       " 9470,\n",
       " 19007,\n",
       " 21702,\n",
       " 13080,\n",
       " 30560,\n",
       " 25911,\n",
       " 55370,\n",
       " 51750,\n",
       " 47430,\n",
       " 16669,\n",
       " 17364,\n",
       " 31568,\n",
       " 39937,\n",
       " 49662,\n",
       " 32935,\n",
       " 57811,\n",
       " 10104,\n",
       " 27598,\n",
       " 20538,\n",
       " 38962,\n",
       " 8050,\n",
       " 41033,\n",
       " 19850,\n",
       " 35001,\n",
       " 30716,\n",
       " 19188,\n",
       " 26873,\n",
       " 50573,\n",
       " 35610,\n",
       " 42213,\n",
       " 51466,\n",
       " 45512,\n",
       " 31537,\n",
       " 39629,\n",
       " 23092,\n",
       " 9313,\n",
       " 53704,\n",
       " 48919,\n",
       " 23068,\n",
       " 32669,\n",
       " 42874,\n",
       " 21176,\n",
       " 39484,\n",
       " 43081,\n",
       " 58277,\n",
       " 7620,\n",
       " 8390,\n",
       " 27988,\n",
       " 56918,\n",
       " 48864,\n",
       " 47398,\n",
       " 25356,\n",
       " 14,\n",
       " 61753,\n",
       " 1256,\n",
       " 34089,\n",
       " 27287,\n",
       " 8085,\n",
       " 57367,\n",
       " 12287,\n",
       " 43140,\n",
       " 175,\n",
       " 1746,\n",
       " 44926,\n",
       " 59256,\n",
       " 57302,\n",
       " 62004,\n",
       " 47576,\n",
       " 37749,\n",
       " 55063,\n",
       " 13868,\n",
       " 47690,\n",
       " 34845]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48087aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d9e66429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 2 2 2\n"
     ]
    }
   ],
   "source": [
    "t1 = (1,1,1)\n",
    "t2 = (2,2,2)\n",
    "\n",
    "def fn(c1,c2,c3,c4,c5,c6):\n",
    "    print(c1,c2,c3,c4,c5,c6)\n",
    "\n",
    "fn(*t1, *t2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b17f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "594acaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ab26642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[n_users, n_items]=[5941, 10303]\n",
      "[n_train, n_test]=[738345, 217212]\n",
      "[n_entities, n_relations, n_triples]=[57541, 8, 90778]\n",
      "[batch_size, batch_size_kg]=[1024, 125]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([3594,\n",
       "  3013,\n",
       "  233,\n",
       "  424,\n",
       "  5855,\n",
       "  5483,\n",
       "  4921,\n",
       "  1873,\n",
       "  5645,\n",
       "  1487,\n",
       "  2006,\n",
       "  3518,\n",
       "  5495,\n",
       "  3615,\n",
       "  5838,\n",
       "  3499,\n",
       "  5176,\n",
       "  138,\n",
       "  2936,\n",
       "  5791,\n",
       "  1849,\n",
       "  1467,\n",
       "  171,\n",
       "  1037,\n",
       "  843,\n",
       "  469,\n",
       "  4526,\n",
       "  5015,\n",
       "  4747,\n",
       "  5107,\n",
       "  2674,\n",
       "  1261,\n",
       "  3156,\n",
       "  3744,\n",
       "  5664,\n",
       "  3774,\n",
       "  303,\n",
       "  4199,\n",
       "  639,\n",
       "  3211,\n",
       "  4695,\n",
       "  3036,\n",
       "  2483,\n",
       "  5148,\n",
       "  5474,\n",
       "  4451,\n",
       "  1696,\n",
       "  3760,\n",
       "  5273,\n",
       "  779,\n",
       "  3327,\n",
       "  1528,\n",
       "  1219,\n",
       "  398,\n",
       "  5891,\n",
       "  71,\n",
       "  863,\n",
       "  2054,\n",
       "  2769,\n",
       "  1911,\n",
       "  3413,\n",
       "  3057,\n",
       "  1723,\n",
       "  3831,\n",
       "  3010,\n",
       "  2652,\n",
       "  4840,\n",
       "  1229,\n",
       "  3130,\n",
       "  4910,\n",
       "  2319,\n",
       "  3277,\n",
       "  2256,\n",
       "  2472,\n",
       "  941,\n",
       "  1122,\n",
       "  3455,\n",
       "  4750,\n",
       "  3769,\n",
       "  2666,\n",
       "  2331,\n",
       "  4188,\n",
       "  3573,\n",
       "  689,\n",
       "  2120,\n",
       "  2464,\n",
       "  2014,\n",
       "  3712,\n",
       "  1495,\n",
       "  1814,\n",
       "  4144,\n",
       "  2445,\n",
       "  4981,\n",
       "  4499,\n",
       "  2971,\n",
       "  762,\n",
       "  3080,\n",
       "  773,\n",
       "  4235,\n",
       "  620,\n",
       "  3139,\n",
       "  2633,\n",
       "  5519,\n",
       "  2010,\n",
       "  3131,\n",
       "  2990,\n",
       "  5385,\n",
       "  2159,\n",
       "  4768,\n",
       "  1820,\n",
       "  4553,\n",
       "  4794,\n",
       "  1597,\n",
       "  1638,\n",
       "  2951,\n",
       "  5906,\n",
       "  1606,\n",
       "  2672,\n",
       "  1088,\n",
       "  909,\n",
       "  679,\n",
       "  2495,\n",
       "  2236,\n",
       "  3956,\n",
       "  5665,\n",
       "  147,\n",
       "  862,\n",
       "  2106,\n",
       "  2177,\n",
       "  4927,\n",
       "  3420,\n",
       "  3002,\n",
       "  3250,\n",
       "  3703,\n",
       "  2138,\n",
       "  1140,\n",
       "  2300,\n",
       "  2151,\n",
       "  3194,\n",
       "  739,\n",
       "  1025,\n",
       "  3921,\n",
       "  4641,\n",
       "  4248,\n",
       "  4161,\n",
       "  5551,\n",
       "  2794,\n",
       "  4114,\n",
       "  1967,\n",
       "  678,\n",
       "  1489,\n",
       "  1992,\n",
       "  1447,\n",
       "  5344,\n",
       "  2560,\n",
       "  5875,\n",
       "  4345,\n",
       "  2639,\n",
       "  1929,\n",
       "  496,\n",
       "  3557,\n",
       "  2190,\n",
       "  1069,\n",
       "  3750,\n",
       "  3251,\n",
       "  5395,\n",
       "  4785,\n",
       "  4708,\n",
       "  4119,\n",
       "  2873,\n",
       "  1280,\n",
       "  440,\n",
       "  1065,\n",
       "  1395,\n",
       "  492,\n",
       "  4418,\n",
       "  3525,\n",
       "  4908,\n",
       "  351,\n",
       "  3903,\n",
       "  4516,\n",
       "  4485,\n",
       "  73,\n",
       "  544,\n",
       "  2507,\n",
       "  5930,\n",
       "  5189,\n",
       "  1132,\n",
       "  3038,\n",
       "  4627,\n",
       "  2232,\n",
       "  780,\n",
       "  2998,\n",
       "  1931,\n",
       "  1104,\n",
       "  5077,\n",
       "  4518,\n",
       "  4186,\n",
       "  903,\n",
       "  1935,\n",
       "  784,\n",
       "  589,\n",
       "  1946,\n",
       "  2554,\n",
       "  3198,\n",
       "  2831,\n",
       "  1160,\n",
       "  2868,\n",
       "  3268,\n",
       "  5110,\n",
       "  1957,\n",
       "  955,\n",
       "  1915,\n",
       "  995,\n",
       "  2571,\n",
       "  5295,\n",
       "  3308,\n",
       "  1247,\n",
       "  5800,\n",
       "  3992,\n",
       "  2821,\n",
       "  5515,\n",
       "  3304,\n",
       "  2404,\n",
       "  1382,\n",
       "  1879,\n",
       "  1611,\n",
       "  5735,\n",
       "  1187,\n",
       "  2508,\n",
       "  557,\n",
       "  4783,\n",
       "  1137,\n",
       "  1766,\n",
       "  1525,\n",
       "  4109,\n",
       "  33,\n",
       "  4398,\n",
       "  1573,\n",
       "  2312,\n",
       "  543,\n",
       "  804,\n",
       "  4960,\n",
       "  1740,\n",
       "  3089,\n",
       "  1002,\n",
       "  1227,\n",
       "  178,\n",
       "  3409,\n",
       "  5504,\n",
       "  2547,\n",
       "  235,\n",
       "  5302,\n",
       "  3405,\n",
       "  549,\n",
       "  1860,\n",
       "  2558,\n",
       "  4241,\n",
       "  3654,\n",
       "  2981,\n",
       "  1652,\n",
       "  603,\n",
       "  1419,\n",
       "  122,\n",
       "  3444,\n",
       "  4733,\n",
       "  1392,\n",
       "  595,\n",
       "  3895,\n",
       "  2355,\n",
       "  4847,\n",
       "  4331,\n",
       "  2699,\n",
       "  4176,\n",
       "  5037,\n",
       "  4010,\n",
       "  5591,\n",
       "  2519,\n",
       "  4616,\n",
       "  657,\n",
       "  3841,\n",
       "  5520,\n",
       "  933,\n",
       "  1888,\n",
       "  2886,\n",
       "  5783,\n",
       "  4239,\n",
       "  629,\n",
       "  540,\n",
       "  5114,\n",
       "  5451,\n",
       "  4799,\n",
       "  5118,\n",
       "  1024,\n",
       "  4752,\n",
       "  3986,\n",
       "  3713,\n",
       "  2670,\n",
       "  1516,\n",
       "  3464,\n",
       "  2615,\n",
       "  992,\n",
       "  5208,\n",
       "  4642,\n",
       "  3095,\n",
       "  2832,\n",
       "  401,\n",
       "  4622,\n",
       "  4439,\n",
       "  1970,\n",
       "  3868,\n",
       "  3723,\n",
       "  5508,\n",
       "  3622,\n",
       "  4529,\n",
       "  2453,\n",
       "  761,\n",
       "  3502,\n",
       "  1497,\n",
       "  4820,\n",
       "  2725,\n",
       "  3200,\n",
       "  2719,\n",
       "  4156,\n",
       "  275,\n",
       "  5732,\n",
       "  5803,\n",
       "  4391,\n",
       "  1455,\n",
       "  3694,\n",
       "  2366,\n",
       "  2504,\n",
       "  535,\n",
       "  959,\n",
       "  298,\n",
       "  5657,\n",
       "  5415,\n",
       "  2100,\n",
       "  5674,\n",
       "  4972,\n",
       "  5934,\n",
       "  3161,\n",
       "  5708,\n",
       "  840,\n",
       "  1829,\n",
       "  1052,\n",
       "  5543,\n",
       "  3226,\n",
       "  520,\n",
       "  2515,\n",
       "  1824,\n",
       "  197,\n",
       "  5746,\n",
       "  876,\n",
       "  3177,\n",
       "  5328,\n",
       "  5507,\n",
       "  5398,\n",
       "  1286,\n",
       "  3901,\n",
       "  2563,\n",
       "  431,\n",
       "  4193,\n",
       "  1649,\n",
       "  356,\n",
       "  3134,\n",
       "  2655,\n",
       "  5842,\n",
       "  5221,\n",
       "  4671,\n",
       "  3430,\n",
       "  5285,\n",
       "  1338,\n",
       "  1281,\n",
       "  4258,\n",
       "  5350,\n",
       "  3870,\n",
       "  152,\n",
       "  4446,\n",
       "  1215,\n",
       "  5607,\n",
       "  98,\n",
       "  3483,\n",
       "  5578,\n",
       "  1057,\n",
       "  2671,\n",
       "  2677,\n",
       "  1106,\n",
       "  1621,\n",
       "  2322,\n",
       "  3938,\n",
       "  3329,\n",
       "  1418,\n",
       "  3657,\n",
       "  2582,\n",
       "  4017,\n",
       "  2881,\n",
       "  4163,\n",
       "  2945,\n",
       "  1204,\n",
       "  2678,\n",
       "  2328,\n",
       "  3606,\n",
       "  1663,\n",
       "  2523,\n",
       "  3068,\n",
       "  1017,\n",
       "  705,\n",
       "  4774,\n",
       "  5023,\n",
       "  2081,\n",
       "  4879,\n",
       "  4123,\n",
       "  251,\n",
       "  5028,\n",
       "  1530,\n",
       "  1858,\n",
       "  4205,\n",
       "  1875,\n",
       "  3672,\n",
       "  738,\n",
       "  901,\n",
       "  3994,\n",
       "  3287,\n",
       "  3959,\n",
       "  923,\n",
       "  253,\n",
       "  3815,\n",
       "  3814,\n",
       "  4164,\n",
       "  188,\n",
       "  1029,\n",
       "  4437,\n",
       "  2090,\n",
       "  3289,\n",
       "  5535,\n",
       "  2423,\n",
       "  4669,\n",
       "  1409,\n",
       "  2332,\n",
       "  1706,\n",
       "  2997,\n",
       "  2527,\n",
       "  3151,\n",
       "  1248,\n",
       "  5466,\n",
       "  4866,\n",
       "  682,\n",
       "  1412,\n",
       "  2087,\n",
       "  2513,\n",
       "  1659,\n",
       "  4776,\n",
       "  5043,\n",
       "  125,\n",
       "  1853,\n",
       "  975,\n",
       "  4550,\n",
       "  5893,\n",
       "  1295,\n",
       "  5596,\n",
       "  946,\n",
       "  3174,\n",
       "  1791,\n",
       "  642,\n",
       "  3823,\n",
       "  2145,\n",
       "  1818,\n",
       "  286,\n",
       "  3206,\n",
       "  3410,\n",
       "  2975,\n",
       "  3474,\n",
       "  1869,\n",
       "  3153,\n",
       "  5000,\n",
       "  912,\n",
       "  5814,\n",
       "  5618,\n",
       "  5939,\n",
       "  1129,\n",
       "  1494,\n",
       "  3919,\n",
       "  5235,\n",
       "  5742,\n",
       "  268,\n",
       "  2314,\n",
       "  4086,\n",
       "  3283,\n",
       "  1832,\n",
       "  5681,\n",
       "  668,\n",
       "  3385,\n",
       "  5856,\n",
       "  5822,\n",
       "  4053,\n",
       "  3710,\n",
       "  5493,\n",
       "  3780,\n",
       "  4998,\n",
       "  1133,\n",
       "  2291,\n",
       "  1754,\n",
       "  3160,\n",
       "  3733,\n",
       "  3314,\n",
       "  4683,\n",
       "  4568,\n",
       "  1253,\n",
       "  4165,\n",
       "  2160,\n",
       "  4486,\n",
       "  2587,\n",
       "  1136,\n",
       "  4892,\n",
       "  2601,\n",
       "  776,\n",
       "  1195,\n",
       "  2233,\n",
       "  5916,\n",
       "  5548,\n",
       "  1540,\n",
       "  5053,\n",
       "  5082,\n",
       "  1589,\n",
       "  2964,\n",
       "  860,\n",
       "  2663,\n",
       "  2735,\n",
       "  185,\n",
       "  726,\n",
       "  765,\n",
       "  709,\n",
       "  5413,\n",
       "  447,\n",
       "  3899,\n",
       "  2259,\n",
       "  754,\n",
       "  3274,\n",
       "  2348,\n",
       "  3747,\n",
       "  5142,\n",
       "  4702,\n",
       "  593,\n",
       "  4106,\n",
       "  786,\n",
       "  1871,\n",
       "  1417,\n",
       "  4432,\n",
       "  1440,\n",
       "  3782,\n",
       "  4271,\n",
       "  113,\n",
       "  3375,\n",
       "  3850,\n",
       "  4699,\n",
       "  5593,\n",
       "  680,\n",
       "  126,\n",
       "  4208,\n",
       "  651,\n",
       "  2338,\n",
       "  1038,\n",
       "  4098,\n",
       "  2583,\n",
       "  3637,\n",
       "  531,\n",
       "  2690,\n",
       "  2302,\n",
       "  5151,\n",
       "  1279,\n",
       "  2620,\n",
       "  4192,\n",
       "  4646,\n",
       "  4503,\n",
       "  2525,\n",
       "  1361,\n",
       "  3381,\n",
       "  2802,\n",
       "  2283,\n",
       "  1778,\n",
       "  2917,\n",
       "  5277,\n",
       "  225,\n",
       "  5282,\n",
       "  978,\n",
       "  2648,\n",
       "  5283,\n",
       "  1050,\n",
       "  1587,\n",
       "  229,\n",
       "  4593,\n",
       "  5025,\n",
       "  5430,\n",
       "  5739,\n",
       "  2263,\n",
       "  2078,\n",
       "  4013,\n",
       "  5060,\n",
       "  4146,\n",
       "  3209,\n",
       "  5695,\n",
       "  2888,\n",
       "  1797,\n",
       "  700,\n",
       "  4790,\n",
       "  5798,\n",
       "  3271,\n",
       "  5777,\n",
       "  1372,\n",
       "  2879,\n",
       "  5184,\n",
       "  647,\n",
       "  371,\n",
       "  1755,\n",
       "  150,\n",
       "  3172,\n",
       "  3467,\n",
       "  5212,\n",
       "  785,\n",
       "  2468,\n",
       "  5627,\n",
       "  3619,\n",
       "  4412,\n",
       "  5313,\n",
       "  1274,\n",
       "  2244,\n",
       "  2905,\n",
       "  353,\n",
       "  2221,\n",
       "  1174,\n",
       "  363,\n",
       "  5652,\n",
       "  4762,\n",
       "  314,\n",
       "  5523,\n",
       "  3214,\n",
       "  2326,\n",
       "  3240,\n",
       "  3162,\n",
       "  127,\n",
       "  1717,\n",
       "  1646,\n",
       "  1894,\n",
       "  3871,\n",
       "  202,\n",
       "  2102,\n",
       "  5165,\n",
       "  5448,\n",
       "  3264,\n",
       "  3419,\n",
       "  621,\n",
       "  78,\n",
       "  4853,\n",
       "  294,\n",
       "  5417,\n",
       "  1112,\n",
       "  2155,\n",
       "  1181,\n",
       "  1647,\n",
       "  5919,\n",
       "  1683,\n",
       "  3097,\n",
       "  4462,\n",
       "  5389,\n",
       "  3922,\n",
       "  1762,\n",
       "  505,\n",
       "  3442,\n",
       "  3800,\n",
       "  1313,\n",
       "  5137,\n",
       "  4565,\n",
       "  5496,\n",
       "  1323,\n",
       "  3217,\n",
       "  2973,\n",
       "  4725,\n",
       "  4589,\n",
       "  4067,\n",
       "  3084,\n",
       "  2200,\n",
       "  4066,\n",
       "  812,\n",
       "  3856,\n",
       "  5802,\n",
       "  4557,\n",
       "  3221,\n",
       "  4071,\n",
       "  1206,\n",
       "  3249,\n",
       "  4379,\n",
       "  3722,\n",
       "  2142,\n",
       "  3480,\n",
       "  4651,\n",
       "  2913,\n",
       "  2031,\n",
       "  2602,\n",
       "  69,\n",
       "  4338,\n",
       "  3370,\n",
       "  4251,\n",
       "  3085,\n",
       "  1249,\n",
       "  5013,\n",
       "  4061,\n",
       "  2840,\n",
       "  5127,\n",
       "  1408,\n",
       "  2862,\n",
       "  1317,\n",
       "  3155,\n",
       "  290,\n",
       "  4806,\n",
       "  4197,\n",
       "  5463,\n",
       "  2764,\n",
       "  5363,\n",
       "  4327,\n",
       "  873,\n",
       "  2110,\n",
       "  3318,\n",
       "  2568,\n",
       "  561,\n",
       "  5145,\n",
       "  4563,\n",
       "  3625,\n",
       "  3031,\n",
       "  2339,\n",
       "  289,\n",
       "  4640,\n",
       "  2019,\n",
       "  3632,\n",
       "  408,\n",
       "  4625,\n",
       "  1328,\n",
       "  605,\n",
       "  3408,\n",
       "  3981,\n",
       "  2986,\n",
       "  1159,\n",
       "  4560,\n",
       "  4180,\n",
       "  2820,\n",
       "  749,\n",
       "  93,\n",
       "  4336,\n",
       "  5223,\n",
       "  4548,\n",
       "  4979,\n",
       "  1517,\n",
       "  383,\n",
       "  4149,\n",
       "  3485,\n",
       "  1900,\n",
       "  1176,\n",
       "  324,\n",
       "  567,\n",
       "  2023,\n",
       "  478,\n",
       "  5456,\n",
       "  1486,\n",
       "  3514,\n",
       "  3454,\n",
       "  1735,\n",
       "  255,\n",
       "  2390,\n",
       "  4299,\n",
       "  4297,\n",
       "  1411,\n",
       "  1428,\n",
       "  14,\n",
       "  2897,\n",
       "  1668,\n",
       "  2386,\n",
       "  5542,\n",
       "  999,\n",
       "  911,\n",
       "  1751,\n",
       "  231,\n",
       "  4680,\n",
       "  1859,\n",
       "  2784,\n",
       "  5616,\n",
       "  1242,\n",
       "  321,\n",
       "  4587,\n",
       "  2280,\n",
       "  3019,\n",
       "  1807,\n",
       "  4172,\n",
       "  5568,\n",
       "  1300,\n",
       "  4293,\n",
       "  3113,\n",
       "  2717,\n",
       "  931,\n",
       "  2890,\n",
       "  5203,\n",
       "  5136,\n",
       "  342,\n",
       "  4152,\n",
       "  5656,\n",
       "  4332,\n",
       "  3934,\n",
       "  714,\n",
       "  2363,\n",
       "  2265,\n",
       "  2237,\n",
       "  2343,\n",
       "  905,\n",
       "  1141,\n",
       "  83,\n",
       "  907,\n",
       "  2603,\n",
       "  2555,\n",
       "  1925,\n",
       "  2529,\n",
       "  5016,\n",
       "  175,\n",
       "  84,\n",
       "  102,\n",
       "  3356,\n",
       "  2596,\n",
       "  463,\n",
       "  5843,\n",
       "  3300,\n",
       "  5276,\n",
       "  1627,\n",
       "  625,\n",
       "  3788,\n",
       "  2205,\n",
       "  4108,\n",
       "  4982,\n",
       "  2055,\n",
       "  4218,\n",
       "  5471,\n",
       "  732,\n",
       "  5888,\n",
       "  547,\n",
       "  2439,\n",
       "  4392,\n",
       "  2219,\n",
       "  3812,\n",
       "  3026,\n",
       "  2224,\n",
       "  2828,\n",
       "  4429,\n",
       "  3941,\n",
       "  3508,\n",
       "  3501,\n",
       "  4530,\n",
       "  4255,\n",
       "  4195,\n",
       "  5236,\n",
       "  1070,\n",
       "  3977,\n",
       "  1258,\n",
       "  796,\n",
       "  4517,\n",
       "  3883,\n",
       "  419,\n",
       "  65,\n",
       "  1399,\n",
       "  4856,\n",
       "  5408,\n",
       "  4947,\n",
       "  2222,\n",
       "  4753,\n",
       "  3849,\n",
       "  20,\n",
       "  4227,\n",
       "  4286,\n",
       "  4984,\n",
       "  2035,\n",
       "  2329,\n",
       "  5294,\n",
       "  2502,\n",
       "  3920,\n",
       "  1224,\n",
       "  2548,\n",
       "  2522,\n",
       "  2032,\n",
       "  3220,\n",
       "  55,\n",
       "  702,\n",
       "  4329,\n",
       "  2704,\n",
       "  733,\n",
       "  2088,\n",
       "  1508,\n",
       "  5447,\n",
       "  134,\n",
       "  2306,\n",
       "  631,\n",
       "  5730,\n",
       "  1785,\n",
       "  1689,\n",
       "  3708,\n",
       "  1940,\n",
       "  1225,\n",
       "  4030,\n",
       "  1296,\n",
       "  3939,\n",
       "  3577,\n",
       "  5600,\n",
       "  1526,\n",
       "  966,\n",
       "  5486,\n",
       "  2104,\n",
       "  2324,\n",
       "  3475,\n",
       "  3916,\n",
       "  2364,\n",
       "  3546,\n",
       "  3611,\n",
       "  1344,\n",
       "  2894,\n",
       "  2039,\n",
       "  2446,\n",
       "  5375,\n",
       "  2466,\n",
       "  209,\n",
       "  61,\n",
       "  4323,\n",
       "  5129,\n",
       "  5007,\n",
       "  2119,\n",
       "  3065,\n",
       "  2001,\n",
       "  5229,\n",
       "  4378,\n",
       "  200,\n",
       "  5799,\n",
       "  5005,\n",
       "  2579,\n",
       "  4253,\n",
       "  4864,\n",
       "  2223,\n",
       "  2270,\n",
       "  3636,\n",
       "  4430,\n",
       "  510,\n",
       "  1424,\n",
       "  5167,\n",
       "  421,\n",
       "  2287,\n",
       "  464,\n",
       "  4584,\n",
       "  3497,\n",
       "  4525,\n",
       "  4703,\n",
       "  2469,\n",
       "  2153,\n",
       "  3012,\n",
       "  4562,\n",
       "  4691,\n",
       "  135,\n",
       "  1298,\n",
       "  1387,\n",
       "  1624,\n",
       "  1442,\n",
       "  3554,\n",
       "  4580,\n",
       "  1926,\n",
       "  3862,\n",
       "  4940,\n",
       "  2062,\n",
       "  747,\n",
       "  4226,\n",
       "  571,\n",
       "  819,\n",
       "  509,\n",
       "  5626,\n",
       "  5163,\n",
       "  1049,\n",
       "  498,\n",
       "  2712,\n",
       "  3523,\n",
       "  479,\n",
       "  5459,\n",
       "  207,\n",
       "  176,\n",
       "  2070,\n",
       "  3008,\n",
       "  5554,\n",
       "  5250,\n",
       "  2965,\n",
       "  2600,\n",
       "  590,\n",
       "  5155,\n",
       "  271,\n",
       "  4244,\n",
       "  2934,\n",
       "  347,\n",
       "  2741,\n",
       "  708,\n",
       "  4488,\n",
       "  1828,\n",
       "  ...],\n",
       " [327,\n",
       "  7705,\n",
       "  786,\n",
       "  2156,\n",
       "  2471,\n",
       "  10113,\n",
       "  10053,\n",
       "  6071,\n",
       "  6285,\n",
       "  657,\n",
       "  2233,\n",
       "  1693,\n",
       "  9674,\n",
       "  6107,\n",
       "  1712,\n",
       "  7902,\n",
       "  7601,\n",
       "  4958,\n",
       "  4777,\n",
       "  9880,\n",
       "  4371,\n",
       "  3361,\n",
       "  3839,\n",
       "  2849,\n",
       "  867,\n",
       "  3922,\n",
       "  4220,\n",
       "  10233,\n",
       "  3256,\n",
       "  3947,\n",
       "  1349,\n",
       "  2069,\n",
       "  2514,\n",
       "  7926,\n",
       "  3376,\n",
       "  2603,\n",
       "  4270,\n",
       "  4120,\n",
       "  2231,\n",
       "  3761,\n",
       "  8156,\n",
       "  1683,\n",
       "  2975,\n",
       "  3508,\n",
       "  6888,\n",
       "  4761,\n",
       "  2400,\n",
       "  9172,\n",
       "  8365,\n",
       "  3444,\n",
       "  6383,\n",
       "  7913,\n",
       "  6153,\n",
       "  6417,\n",
       "  4931,\n",
       "  2508,\n",
       "  4939,\n",
       "  2149,\n",
       "  9502,\n",
       "  8774,\n",
       "  4379,\n",
       "  4003,\n",
       "  7713,\n",
       "  2021,\n",
       "  3761,\n",
       "  6298,\n",
       "  5660,\n",
       "  9952,\n",
       "  4800,\n",
       "  1458,\n",
       "  6781,\n",
       "  3683,\n",
       "  6612,\n",
       "  998,\n",
       "  3564,\n",
       "  7410,\n",
       "  256,\n",
       "  9861,\n",
       "  1478,\n",
       "  96,\n",
       "  1306,\n",
       "  2633,\n",
       "  4433,\n",
       "  5665,\n",
       "  16,\n",
       "  5713,\n",
       "  2932,\n",
       "  4685,\n",
       "  6467,\n",
       "  1853,\n",
       "  1143,\n",
       "  2041,\n",
       "  9449,\n",
       "  7469,\n",
       "  5849,\n",
       "  1200,\n",
       "  1448,\n",
       "  9406,\n",
       "  3788,\n",
       "  6238,\n",
       "  3470,\n",
       "  7827,\n",
       "  68,\n",
       "  120,\n",
       "  7687,\n",
       "  7316,\n",
       "  4712,\n",
       "  5830,\n",
       "  2542,\n",
       "  2018,\n",
       "  114,\n",
       "  5064,\n",
       "  3829,\n",
       "  4893,\n",
       "  4663,\n",
       "  3000,\n",
       "  4883,\n",
       "  2218,\n",
       "  959,\n",
       "  6486,\n",
       "  761,\n",
       "  9147,\n",
       "  9595,\n",
       "  6151,\n",
       "  2330,\n",
       "  9672,\n",
       "  733,\n",
       "  2727,\n",
       "  9281,\n",
       "  4327,\n",
       "  822,\n",
       "  3808,\n",
       "  1922,\n",
       "  3100,\n",
       "  9769,\n",
       "  2408,\n",
       "  253,\n",
       "  9068,\n",
       "  2859,\n",
       "  9351,\n",
       "  2893,\n",
       "  7785,\n",
       "  8148,\n",
       "  7637,\n",
       "  7588,\n",
       "  1123,\n",
       "  10242,\n",
       "  9466,\n",
       "  22,\n",
       "  6664,\n",
       "  1000,\n",
       "  972,\n",
       "  1594,\n",
       "  1851,\n",
       "  8060,\n",
       "  4536,\n",
       "  8938,\n",
       "  1582,\n",
       "  5198,\n",
       "  434,\n",
       "  4750,\n",
       "  5656,\n",
       "  101,\n",
       "  6184,\n",
       "  734,\n",
       "  6581,\n",
       "  1934,\n",
       "  1151,\n",
       "  931,\n",
       "  2761,\n",
       "  1113,\n",
       "  3574,\n",
       "  1457,\n",
       "  3835,\n",
       "  4034,\n",
       "  4296,\n",
       "  9969,\n",
       "  2333,\n",
       "  9015,\n",
       "  2830,\n",
       "  3517,\n",
       "  6715,\n",
       "  8113,\n",
       "  10282,\n",
       "  1444,\n",
       "  6846,\n",
       "  9141,\n",
       "  4796,\n",
       "  5712,\n",
       "  1448,\n",
       "  5445,\n",
       "  3858,\n",
       "  422,\n",
       "  8440,\n",
       "  7212,\n",
       "  2042,\n",
       "  7356,\n",
       "  270,\n",
       "  9376,\n",
       "  4547,\n",
       "  3978,\n",
       "  6160,\n",
       "  3537,\n",
       "  7719,\n",
       "  4322,\n",
       "  651,\n",
       "  4468,\n",
       "  2939,\n",
       "  1439,\n",
       "  6929,\n",
       "  9205,\n",
       "  1374,\n",
       "  8050,\n",
       "  5870,\n",
       "  2964,\n",
       "  727,\n",
       "  8149,\n",
       "  5015,\n",
       "  7321,\n",
       "  6447,\n",
       "  4145,\n",
       "  1314,\n",
       "  1187,\n",
       "  3999,\n",
       "  8501,\n",
       "  3336,\n",
       "  7854,\n",
       "  8076,\n",
       "  8994,\n",
       "  1258,\n",
       "  2680,\n",
       "  7322,\n",
       "  4767,\n",
       "  3466,\n",
       "  6075,\n",
       "  8583,\n",
       "  9694,\n",
       "  8329,\n",
       "  9361,\n",
       "  9974,\n",
       "  4642,\n",
       "  10266,\n",
       "  6554,\n",
       "  501,\n",
       "  4059,\n",
       "  6689,\n",
       "  148,\n",
       "  7104,\n",
       "  694,\n",
       "  2369,\n",
       "  759,\n",
       "  4390,\n",
       "  4079,\n",
       "  186,\n",
       "  408,\n",
       "  3758,\n",
       "  4254,\n",
       "  1232,\n",
       "  9482,\n",
       "  993,\n",
       "  2626,\n",
       "  5686,\n",
       "  10203,\n",
       "  1081,\n",
       "  3787,\n",
       "  5098,\n",
       "  7607,\n",
       "  2348,\n",
       "  127,\n",
       "  5590,\n",
       "  1808,\n",
       "  7560,\n",
       "  3301,\n",
       "  377,\n",
       "  5393,\n",
       "  1455,\n",
       "  16,\n",
       "  3462,\n",
       "  9806,\n",
       "  1585,\n",
       "  3761,\n",
       "  1447,\n",
       "  3112,\n",
       "  1694,\n",
       "  2050,\n",
       "  9799,\n",
       "  2348,\n",
       "  706,\n",
       "  3672,\n",
       "  5540,\n",
       "  1122,\n",
       "  7286,\n",
       "  5163,\n",
       "  6756,\n",
       "  5845,\n",
       "  9461,\n",
       "  6478,\n",
       "  334,\n",
       "  7201,\n",
       "  1795,\n",
       "  2178,\n",
       "  59,\n",
       "  10229,\n",
       "  4344,\n",
       "  5042,\n",
       "  2042,\n",
       "  7307,\n",
       "  3788,\n",
       "  1334,\n",
       "  2440,\n",
       "  8062,\n",
       "  2223,\n",
       "  6071,\n",
       "  339,\n",
       "  6535,\n",
       "  1982,\n",
       "  6158,\n",
       "  7019,\n",
       "  5022,\n",
       "  8587,\n",
       "  803,\n",
       "  825,\n",
       "  6794,\n",
       "  2440,\n",
       "  3060,\n",
       "  648,\n",
       "  9565,\n",
       "  9802,\n",
       "  6655,\n",
       "  2893,\n",
       "  1174,\n",
       "  1922,\n",
       "  5381,\n",
       "  3494,\n",
       "  1429,\n",
       "  1062,\n",
       "  6227,\n",
       "  5418,\n",
       "  9545,\n",
       "  8707,\n",
       "  1801,\n",
       "  9829,\n",
       "  4325,\n",
       "  5763,\n",
       "  664,\n",
       "  1253,\n",
       "  10000,\n",
       "  129,\n",
       "  365,\n",
       "  5345,\n",
       "  7693,\n",
       "  5688,\n",
       "  5242,\n",
       "  7472,\n",
       "  2492,\n",
       "  8837,\n",
       "  7318,\n",
       "  3027,\n",
       "  4256,\n",
       "  6987,\n",
       "  8087,\n",
       "  7651,\n",
       "  4863,\n",
       "  1184,\n",
       "  7718,\n",
       "  2348,\n",
       "  3764,\n",
       "  6777,\n",
       "  5206,\n",
       "  2370,\n",
       "  3906,\n",
       "  357,\n",
       "  3899,\n",
       "  1363,\n",
       "  2760,\n",
       "  3853,\n",
       "  114,\n",
       "  8533,\n",
       "  8658,\n",
       "  10263,\n",
       "  974,\n",
       "  4768,\n",
       "  3512,\n",
       "  4488,\n",
       "  7823,\n",
       "  7494,\n",
       "  2175,\n",
       "  64,\n",
       "  7162,\n",
       "  7902,\n",
       "  1923,\n",
       "  1476,\n",
       "  727,\n",
       "  4907,\n",
       "  3384,\n",
       "  847,\n",
       "  2345,\n",
       "  3261,\n",
       "  8374,\n",
       "  3968,\n",
       "  3072,\n",
       "  1395,\n",
       "  5527,\n",
       "  242,\n",
       "  5186,\n",
       "  4670,\n",
       "  5379,\n",
       "  7921,\n",
       "  9920,\n",
       "  9156,\n",
       "  3246,\n",
       "  9829,\n",
       "  1570,\n",
       "  3684,\n",
       "  727,\n",
       "  5376,\n",
       "  501,\n",
       "  441,\n",
       "  3799,\n",
       "  2550,\n",
       "  4974,\n",
       "  6397,\n",
       "  1451,\n",
       "  22,\n",
       "  5449,\n",
       "  10020,\n",
       "  8869,\n",
       "  2342,\n",
       "  2329,\n",
       "  2342,\n",
       "  7035,\n",
       "  27,\n",
       "  1629,\n",
       "  2367,\n",
       "  3638,\n",
       "  5128,\n",
       "  1560,\n",
       "  1153,\n",
       "  1086,\n",
       "  7767,\n",
       "  230,\n",
       "  7588,\n",
       "  4426,\n",
       "  3640,\n",
       "  8293,\n",
       "  8442,\n",
       "  875,\n",
       "  1667,\n",
       "  1309,\n",
       "  3161,\n",
       "  3053,\n",
       "  3297,\n",
       "  8283,\n",
       "  9122,\n",
       "  2062,\n",
       "  7996,\n",
       "  9414,\n",
       "  456,\n",
       "  2392,\n",
       "  3464,\n",
       "  5854,\n",
       "  3867,\n",
       "  538,\n",
       "  755,\n",
       "  4512,\n",
       "  9937,\n",
       "  1132,\n",
       "  9883,\n",
       "  6044,\n",
       "  927,\n",
       "  7005,\n",
       "  816,\n",
       "  1397,\n",
       "  6942,\n",
       "  7578,\n",
       "  562,\n",
       "  365,\n",
       "  1878,\n",
       "  8588,\n",
       "  2639,\n",
       "  6647,\n",
       "  1717,\n",
       "  3334,\n",
       "  1725,\n",
       "  2452,\n",
       "  5006,\n",
       "  3857,\n",
       "  3674,\n",
       "  2181,\n",
       "  668,\n",
       "  457,\n",
       "  4903,\n",
       "  4109,\n",
       "  1622,\n",
       "  4164,\n",
       "  6125,\n",
       "  7768,\n",
       "  129,\n",
       "  4566,\n",
       "  1071,\n",
       "  4497,\n",
       "  44,\n",
       "  5088,\n",
       "  8368,\n",
       "  548,\n",
       "  4007,\n",
       "  3953,\n",
       "  1339,\n",
       "  3318,\n",
       "  19,\n",
       "  501,\n",
       "  971,\n",
       "  672,\n",
       "  49,\n",
       "  5499,\n",
       "  2533,\n",
       "  5875,\n",
       "  3678,\n",
       "  5523,\n",
       "  8634,\n",
       "  7310,\n",
       "  4727,\n",
       "  8100,\n",
       "  270,\n",
       "  8145,\n",
       "  5900,\n",
       "  2129,\n",
       "  3854,\n",
       "  129,\n",
       "  2322,\n",
       "  6967,\n",
       "  2142,\n",
       "  7704,\n",
       "  9028,\n",
       "  9671,\n",
       "  5721,\n",
       "  3305,\n",
       "  726,\n",
       "  1474,\n",
       "  1856,\n",
       "  3198,\n",
       "  129,\n",
       "  1653,\n",
       "  362,\n",
       "  1009,\n",
       "  1439,\n",
       "  9495,\n",
       "  43,\n",
       "  523,\n",
       "  9061,\n",
       "  10271,\n",
       "  6194,\n",
       "  1566,\n",
       "  6314,\n",
       "  8947,\n",
       "  7415,\n",
       "  1789,\n",
       "  218,\n",
       "  4401,\n",
       "  3372,\n",
       "  703,\n",
       "  4193,\n",
       "  6514,\n",
       "  3521,\n",
       "  9726,\n",
       "  446,\n",
       "  4250,\n",
       "  4484,\n",
       "  1642,\n",
       "  3452,\n",
       "  660,\n",
       "  3195,\n",
       "  530,\n",
       "  1420,\n",
       "  5480,\n",
       "  1448,\n",
       "  9450,\n",
       "  1153,\n",
       "  6026,\n",
       "  2700,\n",
       "  4623,\n",
       "  4245,\n",
       "  3867,\n",
       "  6406,\n",
       "  1455,\n",
       "  3075,\n",
       "  3260,\n",
       "  3627,\n",
       "  162,\n",
       "  16,\n",
       "  2327,\n",
       "  8756,\n",
       "  340,\n",
       "  10294,\n",
       "  4258,\n",
       "  1363,\n",
       "  2203,\n",
       "  759,\n",
       "  8657,\n",
       "  1826,\n",
       "  3096,\n",
       "  4816,\n",
       "  9831,\n",
       "  3215,\n",
       "  9692,\n",
       "  9504,\n",
       "  1091,\n",
       "  3930,\n",
       "  1957,\n",
       "  1706,\n",
       "  3638,\n",
       "  1267,\n",
       "  2302,\n",
       "  6073,\n",
       "  4763,\n",
       "  1962,\n",
       "  1510,\n",
       "  8038,\n",
       "  9740,\n",
       "  1171,\n",
       "  231,\n",
       "  4217,\n",
       "  543,\n",
       "  5806,\n",
       "  1252,\n",
       "  6788,\n",
       "  9498,\n",
       "  3546,\n",
       "  2626,\n",
       "  3250,\n",
       "  4638,\n",
       "  4278,\n",
       "  9687,\n",
       "  241,\n",
       "  6420,\n",
       "  9938,\n",
       "  8409,\n",
       "  3324,\n",
       "  4911,\n",
       "  8443,\n",
       "  6552,\n",
       "  7782,\n",
       "  5235,\n",
       "  1448,\n",
       "  4860,\n",
       "  6395,\n",
       "  129,\n",
       "  5491,\n",
       "  6956,\n",
       "  1361,\n",
       "  5507,\n",
       "  1910,\n",
       "  8650,\n",
       "  48,\n",
       "  2229,\n",
       "  4730,\n",
       "  2581,\n",
       "  1839,\n",
       "  473,\n",
       "  842,\n",
       "  8202,\n",
       "  517,\n",
       "  3662,\n",
       "  6373,\n",
       "  9682,\n",
       "  3208,\n",
       "  9881,\n",
       "  7503,\n",
       "  6819,\n",
       "  9698,\n",
       "  6784,\n",
       "  1689,\n",
       "  2770,\n",
       "  4885,\n",
       "  6840,\n",
       "  1868,\n",
       "  3111,\n",
       "  4708,\n",
       "  8109,\n",
       "  5165,\n",
       "  1743,\n",
       "  851,\n",
       "  4366,\n",
       "  5204,\n",
       "  2033,\n",
       "  1194,\n",
       "  678,\n",
       "  7848,\n",
       "  7590,\n",
       "  804,\n",
       "  7598,\n",
       "  9821,\n",
       "  1194,\n",
       "  215,\n",
       "  4849,\n",
       "  2138,\n",
       "  7386,\n",
       "  3859,\n",
       "  1199,\n",
       "  9050,\n",
       "  3989,\n",
       "  1878,\n",
       "  3106,\n",
       "  1950,\n",
       "  400,\n",
       "  8696,\n",
       "  8065,\n",
       "  9352,\n",
       "  3344,\n",
       "  5529,\n",
       "  2366,\n",
       "  79,\n",
       "  288,\n",
       "  2038,\n",
       "  8776,\n",
       "  4062,\n",
       "  2250,\n",
       "  6712,\n",
       "  1393,\n",
       "  5200,\n",
       "  789,\n",
       "  7237,\n",
       "  2918,\n",
       "  6714,\n",
       "  1560,\n",
       "  1342,\n",
       "  1507,\n",
       "  3901,\n",
       "  8000,\n",
       "  616,\n",
       "  8144,\n",
       "  1019,\n",
       "  1457,\n",
       "  5528,\n",
       "  8903,\n",
       "  596,\n",
       "  9947,\n",
       "  1241,\n",
       "  5633,\n",
       "  789,\n",
       "  434,\n",
       "  6509,\n",
       "  8302,\n",
       "  9897,\n",
       "  421,\n",
       "  7005,\n",
       "  7839,\n",
       "  8219,\n",
       "  7521,\n",
       "  7610,\n",
       "  7190,\n",
       "  8095,\n",
       "  1071,\n",
       "  6164,\n",
       "  904,\n",
       "  1861,\n",
       "  2393,\n",
       "  1104,\n",
       "  3987,\n",
       "  9334,\n",
       "  1387,\n",
       "  7707,\n",
       "  4196,\n",
       "  9553,\n",
       "  9276,\n",
       "  1120,\n",
       "  167,\n",
       "  536,\n",
       "  1338,\n",
       "  1567,\n",
       "  10115,\n",
       "  7049,\n",
       "  2088,\n",
       "  1689,\n",
       "  7237,\n",
       "  5081,\n",
       "  3064,\n",
       "  8084,\n",
       "  4514,\n",
       "  1507,\n",
       "  8308,\n",
       "  2189,\n",
       "  3533,\n",
       "  906,\n",
       "  702,\n",
       "  924,\n",
       "  2712,\n",
       "  1711,\n",
       "  2605,\n",
       "  5462,\n",
       "  1614,\n",
       "  3021,\n",
       "  650,\n",
       "  7867,\n",
       "  2410,\n",
       "  4603,\n",
       "  9087,\n",
       "  4454,\n",
       "  3982,\n",
       "  8693,\n",
       "  4890,\n",
       "  9742,\n",
       "  1573,\n",
       "  4410,\n",
       "  95,\n",
       "  9366,\n",
       "  7520,\n",
       "  1003,\n",
       "  9540,\n",
       "  9464,\n",
       "  2487,\n",
       "  9249,\n",
       "  7374,\n",
       "  3786,\n",
       "  4066,\n",
       "  2079,\n",
       "  436,\n",
       "  1177,\n",
       "  8019,\n",
       "  4104,\n",
       "  6152,\n",
       "  949,\n",
       "  931,\n",
       "  2579,\n",
       "  2299,\n",
       "  8812,\n",
       "  560,\n",
       "  8985,\n",
       "  3313,\n",
       "  6872,\n",
       "  6685,\n",
       "  319,\n",
       "  7362,\n",
       "  10290,\n",
       "  4021,\n",
       "  8767,\n",
       "  1265,\n",
       "  3301,\n",
       "  856,\n",
       "  6949,\n",
       "  8641,\n",
       "  9513,\n",
       "  6152,\n",
       "  10131,\n",
       "  3715,\n",
       "  3863,\n",
       "  5826,\n",
       "  7940,\n",
       "  2457,\n",
       "  3210,\n",
       "  8015,\n",
       "  6501,\n",
       "  7921,\n",
       "  6294,\n",
       "  3568,\n",
       "  1080,\n",
       "  8949,\n",
       "  8932,\n",
       "  9239,\n",
       "  5570,\n",
       "  1812,\n",
       "  7668,\n",
       "  2747,\n",
       "  8087,\n",
       "  9146,\n",
       "  182,\n",
       "  7767,\n",
       "  4868,\n",
       "  6823,\n",
       "  4416,\n",
       "  708,\n",
       "  6072,\n",
       "  5085,\n",
       "  1708,\n",
       "  4118,\n",
       "  10070,\n",
       "  7114,\n",
       "  7302,\n",
       "  3845,\n",
       "  899,\n",
       "  5091,\n",
       "  6531,\n",
       "  1576,\n",
       "  2650,\n",
       "  1304,\n",
       "  1564,\n",
       "  3313,\n",
       "  1659,\n",
       "  2770,\n",
       "  7797,\n",
       "  4148,\n",
       "  2292,\n",
       "  5682,\n",
       "  5698,\n",
       "  9144,\n",
       "  1028,\n",
       "  8745,\n",
       "  5380,\n",
       "  587,\n",
       "  6777,\n",
       "  3802,\n",
       "  8337,\n",
       "  2815,\n",
       "  1914,\n",
       "  9556,\n",
       "  10037,\n",
       "  9974,\n",
       "  2279,\n",
       "  6515,\n",
       "  6943,\n",
       "  7434,\n",
       "  3818,\n",
       "  454,\n",
       "  10055,\n",
       "  4036,\n",
       "  1687,\n",
       "  992,\n",
       "  4924,\n",
       "  4068,\n",
       "  10059,\n",
       "  5058,\n",
       "  10072,\n",
       "  3736,\n",
       "  736,\n",
       "  3032,\n",
       "  10038,\n",
       "  13,\n",
       "  9983,\n",
       "  7415,\n",
       "  5614,\n",
       "  5648,\n",
       "  3167,\n",
       "  984,\n",
       "  2662,\n",
       "  9336,\n",
       "  312,\n",
       "  6529,\n",
       "  6399,\n",
       "  6505,\n",
       "  6603,\n",
       "  1725,\n",
       "  8957,\n",
       "  1451,\n",
       "  5664,\n",
       "  9475,\n",
       "  3347,\n",
       "  939,\n",
       "  551,\n",
       "  436,\n",
       "  5641,\n",
       "  4994,\n",
       "  1591,\n",
       "  3052,\n",
       "  2245,\n",
       "  2947,\n",
       "  4659,\n",
       "  984,\n",
       "  5873,\n",
       "  5066,\n",
       "  5450,\n",
       "  5084,\n",
       "  3369,\n",
       "  926,\n",
       "  759,\n",
       "  846,\n",
       "  2865,\n",
       "  2582,\n",
       "  5024,\n",
       "  1389,\n",
       "  3916,\n",
       "  225,\n",
       "  5834,\n",
       "  9498,\n",
       "  947,\n",
       "  801,\n",
       "  9854,\n",
       "  1671,\n",
       "  8368,\n",
       "  819,\n",
       "  4346,\n",
       "  2450,\n",
       "  3866,\n",
       "  2395,\n",
       "  1633,\n",
       "  297,\n",
       "  4797,\n",
       "  3681,\n",
       "  771,\n",
       "  298,\n",
       "  3302,\n",
       "  4366,\n",
       "  4718,\n",
       "  10202,\n",
       "  3513,\n",
       "  7544,\n",
       "  2393,\n",
       "  2298,\n",
       "  4902,\n",
       "  328,\n",
       "  6837,\n",
       "  ...],\n",
       " [9781,\n",
       "  690,\n",
       "  3841,\n",
       "  5412,\n",
       "  6173,\n",
       "  6919,\n",
       "  755,\n",
       "  7836,\n",
       "  4666,\n",
       "  4122,\n",
       "  9939,\n",
       "  2443,\n",
       "  9049,\n",
       "  8870,\n",
       "  5199,\n",
       "  4069,\n",
       "  1458,\n",
       "  1917,\n",
       "  7803,\n",
       "  804,\n",
       "  9852,\n",
       "  5811,\n",
       "  8933,\n",
       "  9741,\n",
       "  8545,\n",
       "  8918,\n",
       "  632,\n",
       "  3959,\n",
       "  1707,\n",
       "  4748,\n",
       "  234,\n",
       "  8732,\n",
       "  5517,\n",
       "  4098,\n",
       "  4793,\n",
       "  1871,\n",
       "  7051,\n",
       "  7183,\n",
       "  9522,\n",
       "  1928,\n",
       "  1746,\n",
       "  3920,\n",
       "  3221,\n",
       "  6400,\n",
       "  5451,\n",
       "  4450,\n",
       "  2493,\n",
       "  1682,\n",
       "  5912,\n",
       "  1613,\n",
       "  1275,\n",
       "  914,\n",
       "  9618,\n",
       "  2677,\n",
       "  3911,\n",
       "  1595,\n",
       "  3478,\n",
       "  5511,\n",
       "  9937,\n",
       "  1918,\n",
       "  1736,\n",
       "  9592,\n",
       "  2195,\n",
       "  5376,\n",
       "  1159,\n",
       "  9829,\n",
       "  8482,\n",
       "  4716,\n",
       "  647,\n",
       "  571,\n",
       "  8322,\n",
       "  5897,\n",
       "  10201,\n",
       "  6834,\n",
       "  2691,\n",
       "  9802,\n",
       "  3723,\n",
       "  8736,\n",
       "  6931,\n",
       "  1508,\n",
       "  3281,\n",
       "  8421,\n",
       "  894,\n",
       "  2176,\n",
       "  1696,\n",
       "  4773,\n",
       "  3215,\n",
       "  1307,\n",
       "  1829,\n",
       "  6995,\n",
       "  31,\n",
       "  6562,\n",
       "  5944,\n",
       "  1486,\n",
       "  9719,\n",
       "  1179,\n",
       "  9277,\n",
       "  2108,\n",
       "  7010,\n",
       "  9803,\n",
       "  8025,\n",
       "  6813,\n",
       "  1912,\n",
       "  4282,\n",
       "  1677,\n",
       "  2355,\n",
       "  2555,\n",
       "  5553,\n",
       "  2795,\n",
       "  3132,\n",
       "  3695,\n",
       "  4867,\n",
       "  8279,\n",
       "  4258,\n",
       "  8641,\n",
       "  8781,\n",
       "  5420,\n",
       "  721,\n",
       "  639,\n",
       "  8777,\n",
       "  3653,\n",
       "  4678,\n",
       "  3781,\n",
       "  1876,\n",
       "  5832,\n",
       "  782,\n",
       "  9978,\n",
       "  7050,\n",
       "  5843,\n",
       "  2255,\n",
       "  7707,\n",
       "  9073,\n",
       "  5760,\n",
       "  5625,\n",
       "  6715,\n",
       "  8211,\n",
       "  279,\n",
       "  1840,\n",
       "  4067,\n",
       "  6863,\n",
       "  6385,\n",
       "  10094,\n",
       "  4981,\n",
       "  8739,\n",
       "  2464,\n",
       "  4632,\n",
       "  824,\n",
       "  3119,\n",
       "  6297,\n",
       "  8187,\n",
       "  1185,\n",
       "  4395,\n",
       "  9008,\n",
       "  9061,\n",
       "  1910,\n",
       "  3915,\n",
       "  5612,\n",
       "  611,\n",
       "  9700,\n",
       "  2809,\n",
       "  8450,\n",
       "  4785,\n",
       "  4511,\n",
       "  352,\n",
       "  10105,\n",
       "  1464,\n",
       "  4358,\n",
       "  3794,\n",
       "  6005,\n",
       "  5993,\n",
       "  7355,\n",
       "  5349,\n",
       "  3566,\n",
       "  2868,\n",
       "  3301,\n",
       "  9199,\n",
       "  6175,\n",
       "  7019,\n",
       "  7559,\n",
       "  3374,\n",
       "  8688,\n",
       "  3062,\n",
       "  5807,\n",
       "  1367,\n",
       "  7400,\n",
       "  8661,\n",
       "  9866,\n",
       "  4783,\n",
       "  5294,\n",
       "  9126,\n",
       "  3489,\n",
       "  1938,\n",
       "  1328,\n",
       "  4525,\n",
       "  1145,\n",
       "  1255,\n",
       "  10258,\n",
       "  5759,\n",
       "  8483,\n",
       "  673,\n",
       "  7019,\n",
       "  8857,\n",
       "  9259,\n",
       "  6831,\n",
       "  489,\n",
       "  8045,\n",
       "  5521,\n",
       "  6341,\n",
       "  1469,\n",
       "  2093,\n",
       "  10196,\n",
       "  3492,\n",
       "  7228,\n",
       "  783,\n",
       "  8338,\n",
       "  8917,\n",
       "  8700,\n",
       "  1071,\n",
       "  5810,\n",
       "  8365,\n",
       "  130,\n",
       "  4778,\n",
       "  9871,\n",
       "  8076,\n",
       "  1722,\n",
       "  2065,\n",
       "  3903,\n",
       "  5262,\n",
       "  2324,\n",
       "  3415,\n",
       "  3841,\n",
       "  9847,\n",
       "  8074,\n",
       "  9117,\n",
       "  6038,\n",
       "  7147,\n",
       "  4734,\n",
       "  2516,\n",
       "  3635,\n",
       "  305,\n",
       "  9810,\n",
       "  3336,\n",
       "  193,\n",
       "  733,\n",
       "  856,\n",
       "  5143,\n",
       "  2765,\n",
       "  7976,\n",
       "  2909,\n",
       "  5571,\n",
       "  8491,\n",
       "  2538,\n",
       "  2072,\n",
       "  8337,\n",
       "  3033,\n",
       "  744,\n",
       "  1976,\n",
       "  604,\n",
       "  6748,\n",
       "  5945,\n",
       "  382,\n",
       "  3900,\n",
       "  2682,\n",
       "  8776,\n",
       "  3697,\n",
       "  6477,\n",
       "  3806,\n",
       "  659,\n",
       "  1838,\n",
       "  8026,\n",
       "  8685,\n",
       "  1033,\n",
       "  3069,\n",
       "  5924,\n",
       "  4018,\n",
       "  9122,\n",
       "  900,\n",
       "  9518,\n",
       "  3834,\n",
       "  4950,\n",
       "  7212,\n",
       "  2774,\n",
       "  10246,\n",
       "  9140,\n",
       "  1929,\n",
       "  5502,\n",
       "  4698,\n",
       "  5235,\n",
       "  5887,\n",
       "  7420,\n",
       "  3974,\n",
       "  6537,\n",
       "  8359,\n",
       "  7024,\n",
       "  1976,\n",
       "  180,\n",
       "  7695,\n",
       "  7375,\n",
       "  6019,\n",
       "  2791,\n",
       "  1452,\n",
       "  8189,\n",
       "  6357,\n",
       "  6147,\n",
       "  5482,\n",
       "  6207,\n",
       "  8019,\n",
       "  2051,\n",
       "  8517,\n",
       "  6543,\n",
       "  1229,\n",
       "  5761,\n",
       "  2651,\n",
       "  8323,\n",
       "  8166,\n",
       "  3934,\n",
       "  7901,\n",
       "  8967,\n",
       "  9080,\n",
       "  4798,\n",
       "  6734,\n",
       "  1026,\n",
       "  4633,\n",
       "  713,\n",
       "  9226,\n",
       "  5794,\n",
       "  2553,\n",
       "  8588,\n",
       "  9251,\n",
       "  7666,\n",
       "  10218,\n",
       "  3662,\n",
       "  8876,\n",
       "  5066,\n",
       "  6695,\n",
       "  637,\n",
       "  226,\n",
       "  10230,\n",
       "  6433,\n",
       "  9896,\n",
       "  1239,\n",
       "  7878,\n",
       "  7930,\n",
       "  4789,\n",
       "  2843,\n",
       "  8409,\n",
       "  4648,\n",
       "  5747,\n",
       "  6003,\n",
       "  8218,\n",
       "  9449,\n",
       "  2340,\n",
       "  9960,\n",
       "  912,\n",
       "  6245,\n",
       "  1494,\n",
       "  9051,\n",
       "  7570,\n",
       "  5490,\n",
       "  5172,\n",
       "  2665,\n",
       "  5844,\n",
       "  9899,\n",
       "  2872,\n",
       "  10117,\n",
       "  2818,\n",
       "  9801,\n",
       "  2299,\n",
       "  9594,\n",
       "  4985,\n",
       "  6778,\n",
       "  1477,\n",
       "  2884,\n",
       "  866,\n",
       "  3182,\n",
       "  9592,\n",
       "  3194,\n",
       "  4213,\n",
       "  5528,\n",
       "  9974,\n",
       "  5555,\n",
       "  3685,\n",
       "  5447,\n",
       "  3930,\n",
       "  8712,\n",
       "  4974,\n",
       "  8754,\n",
       "  7512,\n",
       "  4663,\n",
       "  8303,\n",
       "  7869,\n",
       "  9516,\n",
       "  2970,\n",
       "  4358,\n",
       "  8147,\n",
       "  3738,\n",
       "  9381,\n",
       "  5133,\n",
       "  1988,\n",
       "  7658,\n",
       "  6715,\n",
       "  6987,\n",
       "  5635,\n",
       "  6271,\n",
       "  5889,\n",
       "  5334,\n",
       "  7356,\n",
       "  198,\n",
       "  9427,\n",
       "  9078,\n",
       "  7609,\n",
       "  2094,\n",
       "  3059,\n",
       "  4624,\n",
       "  5903,\n",
       "  2677,\n",
       "  6200,\n",
       "  6712,\n",
       "  2982,\n",
       "  5679,\n",
       "  3571,\n",
       "  5149,\n",
       "  51,\n",
       "  3248,\n",
       "  1274,\n",
       "  2109,\n",
       "  10046,\n",
       "  1986,\n",
       "  547,\n",
       "  3291,\n",
       "  2021,\n",
       "  4357,\n",
       "  6473,\n",
       "  2125,\n",
       "  1231,\n",
       "  8230,\n",
       "  6290,\n",
       "  9858,\n",
       "  10112,\n",
       "  5957,\n",
       "  670,\n",
       "  9796,\n",
       "  7133,\n",
       "  1036,\n",
       "  7046,\n",
       "  7972,\n",
       "  9803,\n",
       "  7398,\n",
       "  9092,\n",
       "  1984,\n",
       "  5444,\n",
       "  5826,\n",
       "  5605,\n",
       "  7137,\n",
       "  8460,\n",
       "  4869,\n",
       "  9084,\n",
       "  2785,\n",
       "  1987,\n",
       "  7905,\n",
       "  8728,\n",
       "  2849,\n",
       "  8779,\n",
       "  4517,\n",
       "  4217,\n",
       "  8104,\n",
       "  561,\n",
       "  188,\n",
       "  5109,\n",
       "  7357,\n",
       "  4721,\n",
       "  9227,\n",
       "  3561,\n",
       "  10090,\n",
       "  9216,\n",
       "  7419,\n",
       "  6616,\n",
       "  6676,\n",
       "  2687,\n",
       "  9452,\n",
       "  9738,\n",
       "  4369,\n",
       "  10135,\n",
       "  9619,\n",
       "  4629,\n",
       "  1630,\n",
       "  840,\n",
       "  7150,\n",
       "  2096,\n",
       "  9717,\n",
       "  2465,\n",
       "  9743,\n",
       "  7027,\n",
       "  6752,\n",
       "  4524,\n",
       "  2521,\n",
       "  3076,\n",
       "  1161,\n",
       "  6683,\n",
       "  8651,\n",
       "  9304,\n",
       "  683,\n",
       "  14,\n",
       "  9950,\n",
       "  3818,\n",
       "  6956,\n",
       "  8634,\n",
       "  5311,\n",
       "  4087,\n",
       "  8491,\n",
       "  2547,\n",
       "  6029,\n",
       "  7114,\n",
       "  8972,\n",
       "  2355,\n",
       "  7439,\n",
       "  3783,\n",
       "  949,\n",
       "  5161,\n",
       "  5959,\n",
       "  5935,\n",
       "  3587,\n",
       "  2392,\n",
       "  5760,\n",
       "  3255,\n",
       "  3918,\n",
       "  7305,\n",
       "  7606,\n",
       "  872,\n",
       "  10080,\n",
       "  2453,\n",
       "  2983,\n",
       "  7660,\n",
       "  559,\n",
       "  10149,\n",
       "  5692,\n",
       "  8024,\n",
       "  955,\n",
       "  6642,\n",
       "  9083,\n",
       "  4658,\n",
       "  8651,\n",
       "  3100,\n",
       "  3283,\n",
       "  2922,\n",
       "  15,\n",
       "  7195,\n",
       "  9571,\n",
       "  7873,\n",
       "  4455,\n",
       "  7656,\n",
       "  7983,\n",
       "  8749,\n",
       "  5798,\n",
       "  6858,\n",
       "  7448,\n",
       "  5165,\n",
       "  10246,\n",
       "  557,\n",
       "  3791,\n",
       "  7409,\n",
       "  9770,\n",
       "  5211,\n",
       "  6223,\n",
       "  3913,\n",
       "  2131,\n",
       "  672,\n",
       "  6041,\n",
       "  3647,\n",
       "  343,\n",
       "  2035,\n",
       "  9016,\n",
       "  4965,\n",
       "  5731,\n",
       "  3590,\n",
       "  5747,\n",
       "  1242,\n",
       "  6020,\n",
       "  10158,\n",
       "  2048,\n",
       "  9640,\n",
       "  287,\n",
       "  4608,\n",
       "  3129,\n",
       "  2887,\n",
       "  3798,\n",
       "  5722,\n",
       "  1852,\n",
       "  2891,\n",
       "  10165,\n",
       "  1169,\n",
       "  1468,\n",
       "  1910,\n",
       "  7041,\n",
       "  1232,\n",
       "  9269,\n",
       "  6193,\n",
       "  9631,\n",
       "  2698,\n",
       "  2013,\n",
       "  10027,\n",
       "  4153,\n",
       "  7730,\n",
       "  8816,\n",
       "  286,\n",
       "  9345,\n",
       "  7119,\n",
       "  7947,\n",
       "  4790,\n",
       "  109,\n",
       "  5930,\n",
       "  6526,\n",
       "  261,\n",
       "  6240,\n",
       "  4785,\n",
       "  10171,\n",
       "  7781,\n",
       "  7992,\n",
       "  3852,\n",
       "  9735,\n",
       "  9089,\n",
       "  4241,\n",
       "  665,\n",
       "  7013,\n",
       "  5393,\n",
       "  1134,\n",
       "  4399,\n",
       "  6791,\n",
       "  6945,\n",
       "  6627,\n",
       "  516,\n",
       "  716,\n",
       "  3618,\n",
       "  1691,\n",
       "  2305,\n",
       "  2119,\n",
       "  110,\n",
       "  7309,\n",
       "  6065,\n",
       "  5056,\n",
       "  1898,\n",
       "  8282,\n",
       "  8810,\n",
       "  2465,\n",
       "  9566,\n",
       "  4912,\n",
       "  4635,\n",
       "  8654,\n",
       "  2345,\n",
       "  1281,\n",
       "  9252,\n",
       "  211,\n",
       "  1174,\n",
       "  7519,\n",
       "  9136,\n",
       "  780,\n",
       "  417,\n",
       "  5734,\n",
       "  3434,\n",
       "  9762,\n",
       "  3102,\n",
       "  5750,\n",
       "  6885,\n",
       "  2942,\n",
       "  7948,\n",
       "  214,\n",
       "  1997,\n",
       "  1221,\n",
       "  3377,\n",
       "  7379,\n",
       "  9170,\n",
       "  4701,\n",
       "  3204,\n",
       "  5482,\n",
       "  4524,\n",
       "  7434,\n",
       "  6871,\n",
       "  1440,\n",
       "  7003,\n",
       "  788,\n",
       "  6459,\n",
       "  2652,\n",
       "  6885,\n",
       "  6001,\n",
       "  2114,\n",
       "  981,\n",
       "  10278,\n",
       "  6651,\n",
       "  4958,\n",
       "  4407,\n",
       "  6291,\n",
       "  801,\n",
       "  10034,\n",
       "  5728,\n",
       "  1324,\n",
       "  1964,\n",
       "  655,\n",
       "  4420,\n",
       "  7337,\n",
       "  3798,\n",
       "  8930,\n",
       "  3574,\n",
       "  3849,\n",
       "  9753,\n",
       "  1647,\n",
       "  5210,\n",
       "  2607,\n",
       "  211,\n",
       "  9358,\n",
       "  5861,\n",
       "  3804,\n",
       "  5328,\n",
       "  362,\n",
       "  9683,\n",
       "  4192,\n",
       "  2708,\n",
       "  2215,\n",
       "  7017,\n",
       "  8870,\n",
       "  9734,\n",
       "  7978,\n",
       "  3164,\n",
       "  702,\n",
       "  5643,\n",
       "  3185,\n",
       "  3740,\n",
       "  3520,\n",
       "  542,\n",
       "  3221,\n",
       "  10244,\n",
       "  4232,\n",
       "  7082,\n",
       "  7192,\n",
       "  2031,\n",
       "  6197,\n",
       "  1064,\n",
       "  791,\n",
       "  3313,\n",
       "  7719,\n",
       "  9374,\n",
       "  9110,\n",
       "  471,\n",
       "  6142,\n",
       "  2773,\n",
       "  8275,\n",
       "  5442,\n",
       "  5306,\n",
       "  6156,\n",
       "  8350,\n",
       "  4042,\n",
       "  9111,\n",
       "  2400,\n",
       "  1943,\n",
       "  4457,\n",
       "  3163,\n",
       "  1726,\n",
       "  3651,\n",
       "  1903,\n",
       "  529,\n",
       "  3978,\n",
       "  8603,\n",
       "  49,\n",
       "  8960,\n",
       "  5051,\n",
       "  2738,\n",
       "  9909,\n",
       "  1984,\n",
       "  5500,\n",
       "  2729,\n",
       "  9979,\n",
       "  3775,\n",
       "  2417,\n",
       "  6254,\n",
       "  3140,\n",
       "  3434,\n",
       "  10289,\n",
       "  3728,\n",
       "  10166,\n",
       "  4971,\n",
       "  8108,\n",
       "  944,\n",
       "  3545,\n",
       "  2121,\n",
       "  6310,\n",
       "  5271,\n",
       "  483,\n",
       "  4048,\n",
       "  6847,\n",
       "  8047,\n",
       "  3661,\n",
       "  8131,\n",
       "  5007,\n",
       "  2848,\n",
       "  727,\n",
       "  864,\n",
       "  1605,\n",
       "  4368,\n",
       "  1141,\n",
       "  41,\n",
       "  5985,\n",
       "  2412,\n",
       "  2307,\n",
       "  6741,\n",
       "  9375,\n",
       "  1370,\n",
       "  7556,\n",
       "  9528,\n",
       "  1695,\n",
       "  7451,\n",
       "  5226,\n",
       "  8629,\n",
       "  427,\n",
       "  10284,\n",
       "  2855,\n",
       "  1772,\n",
       "  9492,\n",
       "  3635,\n",
       "  8563,\n",
       "  7854,\n",
       "  8569,\n",
       "  1155,\n",
       "  10091,\n",
       "  10234,\n",
       "  2395,\n",
       "  2568,\n",
       "  1222,\n",
       "  3040,\n",
       "  9280,\n",
       "  2711,\n",
       "  2523,\n",
       "  6519,\n",
       "  9636,\n",
       "  7759,\n",
       "  9050,\n",
       "  9258,\n",
       "  4906,\n",
       "  9938,\n",
       "  1327,\n",
       "  2606,\n",
       "  1630,\n",
       "  5773,\n",
       "  1960,\n",
       "  5391,\n",
       "  2188,\n",
       "  2753,\n",
       "  50,\n",
       "  4868,\n",
       "  7716,\n",
       "  4371,\n",
       "  6824,\n",
       "  5641,\n",
       "  2756,\n",
       "  176,\n",
       "  5062,\n",
       "  8915,\n",
       "  3668,\n",
       "  4082,\n",
       "  1889,\n",
       "  3695,\n",
       "  1282,\n",
       "  4432,\n",
       "  1156,\n",
       "  3301,\n",
       "  5261,\n",
       "  8907,\n",
       "  8062,\n",
       "  3238,\n",
       "  7057,\n",
       "  6682,\n",
       "  10298,\n",
       "  4359,\n",
       "  1849,\n",
       "  10245,\n",
       "  5014,\n",
       "  3433,\n",
       "  7337,\n",
       "  4201,\n",
       "  6423,\n",
       "  3992,\n",
       "  2282,\n",
       "  6855,\n",
       "  487,\n",
       "  10036,\n",
       "  6177,\n",
       "  1034,\n",
       "  10046,\n",
       "  8852,\n",
       "  6105,\n",
       "  9522,\n",
       "  10229,\n",
       "  7455,\n",
       "  6600,\n",
       "  5735,\n",
       "  6353,\n",
       "  7271,\n",
       "  8325,\n",
       "  6842,\n",
       "  8001,\n",
       "  10021,\n",
       "  1472,\n",
       "  5146,\n",
       "  7958,\n",
       "  5956,\n",
       "  2857,\n",
       "  1016,\n",
       "  1987,\n",
       "  4560,\n",
       "  5013,\n",
       "  3439,\n",
       "  1675,\n",
       "  9346,\n",
       "  4672,\n",
       "  4945,\n",
       "  5583,\n",
       "  5844,\n",
       "  7421,\n",
       "  2543,\n",
       "  3575,\n",
       "  5666,\n",
       "  8401,\n",
       "  4886,\n",
       "  9473,\n",
       "  8892,\n",
       "  4582,\n",
       "  413,\n",
       "  7758,\n",
       "  2441,\n",
       "  10058,\n",
       "  8307,\n",
       "  2049,\n",
       "  4708,\n",
       "  9929,\n",
       "  1520,\n",
       "  3104,\n",
       "  1680,\n",
       "  2297,\n",
       "  9808,\n",
       "  4020,\n",
       "  7830,\n",
       "  6829,\n",
       "  10050,\n",
       "  3189,\n",
       "  8873,\n",
       "  9117,\n",
       "  739,\n",
       "  5401,\n",
       "  8825,\n",
       "  7349,\n",
       "  2421,\n",
       "  9704,\n",
       "  2865,\n",
       "  8344,\n",
       "  6601,\n",
       "  4181,\n",
       "  8007,\n",
       "  833,\n",
       "  660,\n",
       "  4824,\n",
       "  5104,\n",
       "  3748,\n",
       "  309,\n",
       "  7425,\n",
       "  213,\n",
       "  16,\n",
       "  2398,\n",
       "  7061,\n",
       "  3079,\n",
       "  9421,\n",
       "  3910,\n",
       "  4856,\n",
       "  7584,\n",
       "  8638,\n",
       "  2273,\n",
       "  2785,\n",
       "  2880,\n",
       "  4634,\n",
       "  5074,\n",
       "  411,\n",
       "  5618,\n",
       "  9281,\n",
       "  3443,\n",
       "  10062,\n",
       "  7267,\n",
       "  2803,\n",
       "  6924,\n",
       "  6102,\n",
       "  6364,\n",
       "  2515,\n",
       "  3966,\n",
       "  7239,\n",
       "  3391,\n",
       "  7020,\n",
       "  4396,\n",
       "  5057,\n",
       "  221,\n",
       "  3915,\n",
       "  4779,\n",
       "  1138,\n",
       "  7341,\n",
       "  7852,\n",
       "  ...])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Created on Dec 18, 2018\n",
    "Tensorflow Implementation of Knowledge Graph Attention Network (KGAT) model in:\n",
    "Wang Xiang et al. KGAT: Knowledge Graph Attention Network for Recommendation. In KDD 2019.\n",
    "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
    "'''\n",
    "import collections\n",
    "import numpy as np\n",
    "import random as rd\n",
    "\n",
    "class Data(object):\n",
    "    def __init__(self, args, path):\n",
    "        self.path = path\n",
    "        self.args = args\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "\n",
    "        train_file = path + '/train.txt'\n",
    "        test_file = path + '/test.txt'\n",
    "\n",
    "        kg_file = path + '/kg_final.txt'\n",
    "\n",
    "        # ----------get number of users and items & then load rating data from train_file & test_file------------.\n",
    "        self.n_train, self.n_test = 0, 0\n",
    "        self.n_users, self.n_items = 0, 0\n",
    "\n",
    "        self.train_data, self.train_user_dict = self._load_ratings(train_file)\n",
    "        self.test_data, self.test_user_dict = self._load_ratings(test_file)\n",
    "        self.exist_users = self.train_user_dict.keys()\n",
    "\n",
    "        self._statistic_ratings()\n",
    "\n",
    "        # ----------get number of entities and relations & then load kg data from kg_file ------------.\n",
    "        self.n_relations, self.n_entities, self.n_triples = 0, 0, 0\n",
    "        self.kg_data, self.kg_dict, self.relation_dict = self._load_kg(kg_file)\n",
    "\n",
    "        # ----------print the basic info about the dataset-------------.\n",
    "        self.batch_size_kg = self.n_triples // (self.n_train // self.batch_size)\n",
    "        self._print_data_info()\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_ratings(self, file_name):\n",
    "        user_dict = dict()\n",
    "        inter_mat = list()\n",
    "\n",
    "        lines = open(file_name, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmps = l.strip()\n",
    "            inters = [int(i) for i in tmps.split(' ')]\n",
    "\n",
    "            u_id, pos_ids = inters[0], inters[1:]\n",
    "            pos_ids = list(set(pos_ids))\n",
    "\n",
    "            for i_id in pos_ids:\n",
    "                inter_mat.append([u_id, i_id])\n",
    "\n",
    "            if len(pos_ids) > 0:\n",
    "                user_dict[u_id] = pos_ids\n",
    "        return np.array(inter_mat), user_dict\n",
    "\n",
    "    def _statistic_ratings(self):\n",
    "        self.n_users = max(max(self.train_data[:, 0]), max(self.test_data[:, 0])) + 1\n",
    "        self.n_items = max(max(self.train_data[:, 1]), max(self.test_data[:, 1])) + 1\n",
    "        self.n_train = len(self.train_data)\n",
    "        self.n_test = len(self.test_data)\n",
    "\n",
    "    # reading train & test interaction data.\n",
    "    def _load_kg(self, file_name):\n",
    "        def _construct_kg(kg_np):\n",
    "            kg = collections.defaultdict(list)\n",
    "            rd = collections.defaultdict(list)\n",
    "\n",
    "            for head, relation, tail in kg_np:\n",
    "                kg[head].append((tail, relation))\n",
    "                rd[relation].append((head, tail))\n",
    "            return kg, rd\n",
    "\n",
    "        kg_np = np.loadtxt(file_name, dtype=np.int32)\n",
    "        kg_np = np.unique(kg_np, axis=0)\n",
    "\n",
    "        # self.n_relations = len(set(kg_np[:, 1]))\n",
    "        # self.n_entities = len(set(kg_np[:, 0]) | set(kg_np[:, 2]))\n",
    "        self.n_relations = max(kg_np[:, 1]) + 1\n",
    "        self.n_entities = max(max(kg_np[:, 0]), max(kg_np[:, 2])) + 1\n",
    "        self.n_triples = len(kg_np)\n",
    "\n",
    "        kg_dict, relation_dict = _construct_kg(kg_np)\n",
    "\n",
    "        return kg_np, kg_dict, relation_dict\n",
    "\n",
    "    def _print_data_info(self):\n",
    "        print('[n_users, n_items]=[%d, %d]' % (self.n_users, self.n_items))\n",
    "        print('[n_train, n_test]=[%d, %d]' % (self.n_train, self.n_test))\n",
    "        print('[n_entities, n_relations, n_triples]=[%d, %d, %d]' % (self.n_entities, self.n_relations, self.n_triples))\n",
    "        print('[batch_size, batch_size_kg]=[%d, %d]' % (self.batch_size, self.batch_size_kg))\n",
    "\n",
    "    def _generate_train_cf_batch(self):\n",
    "        if self.batch_size <= self.n_users:\n",
    "            users = rd.sample(self.exist_users, self.batch_size)\n",
    "        else:\n",
    "            users = [rd.choice(self.exist_users) for _ in range(self.batch_size)]\n",
    "\n",
    "        def sample_pos_items_for_u(u, num):\n",
    "            pos_items = self.train_user_dict[u]\n",
    "            n_pos_items = len(pos_items)\n",
    "            pos_batch = []\n",
    "            while True:\n",
    "                if len(pos_batch) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "                pos_i_id = pos_items[pos_id]\n",
    "\n",
    "                if pos_i_id not in pos_batch:\n",
    "                    pos_batch.append(pos_i_id)\n",
    "            return pos_batch\n",
    "\n",
    "        def sample_neg_items_for_u(u, num):\n",
    "            neg_items = []\n",
    "            while True:\n",
    "                if len(neg_items) == num: break\n",
    "                neg_i_id = np.random.randint(low=0, high=self.n_items,size=1)[0]\n",
    "\n",
    "                if neg_i_id not in self.train_user_dict[u] and neg_i_id not in neg_items:\n",
    "                    neg_items.append(neg_i_id)\n",
    "            return neg_items\n",
    "\n",
    "        pos_items, neg_items = [], []\n",
    "        for u in users:\n",
    "            pos_items += sample_pos_items_for_u(u, 1)\n",
    "            neg_items += sample_neg_items_for_u(u, 1)\n",
    "\n",
    "        return users, pos_items, neg_items\n",
    "\n",
    "    def get_sparsity_split(self):\n",
    "        try:\n",
    "            split_uids, split_state = [], []\n",
    "            lines = open(self.path + '/sparsity.split', 'r').readlines()\n",
    "\n",
    "            for idx, line in enumerate(lines):\n",
    "                if idx % 2 == 0:\n",
    "                    split_state.append(line.strip())\n",
    "                    print(line.strip())\n",
    "                else:\n",
    "                    split_uids.append([int(uid) for uid in line.strip().split(' ')])\n",
    "            print('get sparsity split.')\n",
    "\n",
    "        except Exception:\n",
    "            split_uids, split_state = self.create_sparsity_split()\n",
    "            f = open(self.path + '/sparsity.split', 'w')\n",
    "            for idx in range(len(split_state)):\n",
    "                f.write(split_state[idx] + '\\n')\n",
    "                f.write(' '.join([str(uid) for uid in split_uids[idx]]) + '\\n')\n",
    "            print('create sparsity split.')\n",
    "\n",
    "        return split_uids, split_state\n",
    "\n",
    "\n",
    "\n",
    "    def create_sparsity_split(self):\n",
    "        all_users_to_test = list(self.test_user_dict.keys())\n",
    "        user_n_iid = dict()\n",
    "\n",
    "        # generate a dictionary to store (key=n_iids, value=a list of uid).\n",
    "        for uid in all_users_to_test:\n",
    "            train_iids = self.train_user_dict[uid]\n",
    "            test_iids = self.test_user_dict[uid]\n",
    "\n",
    "            n_iids = len(train_iids) + len(test_iids)\n",
    "\n",
    "            if n_iids not in user_n_iid.keys():\n",
    "                user_n_iid[n_iids] = [uid]\n",
    "            else:\n",
    "                user_n_iid[n_iids].append(uid)\n",
    "        split_uids = list()\n",
    "\n",
    "        # split the whole user set into four subset.\n",
    "        temp = []\n",
    "        count = 1\n",
    "        fold = 4\n",
    "        n_count = (self.n_train + self.n_test)\n",
    "        n_rates = 0\n",
    "\n",
    "        split_state = []\n",
    "        for idx, n_iids in enumerate(sorted(user_n_iid)):\n",
    "            temp += user_n_iid[n_iids]\n",
    "            n_rates += n_iids * len(user_n_iid[n_iids])\n",
    "            n_count -= n_iids * len(user_n_iid[n_iids])\n",
    "\n",
    "            if n_rates >= count * 0.25 * (self.n_train + self.n_test):\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' %(n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "                temp = []\n",
    "                n_rates = 0\n",
    "                fold -= 1\n",
    "\n",
    "            if idx == len(user_n_iid.keys()) - 1 or n_count == 0:\n",
    "                split_uids.append(temp)\n",
    "\n",
    "                state = '#inter per user<=[%d], #users=[%d], #all rates=[%d]' % (n_iids, len(temp), n_rates)\n",
    "                split_state.append(state)\n",
    "                print(state)\n",
    "\n",
    "\n",
    "        return split_uids, split_state\n",
    "    \n",
    "    \n",
    "data = Data(args=args, path=args.data_path + args.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f847ed80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0373837947845459\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "batched_data = data._generate_train_cf_batch()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ef626606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dcec3d83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batched_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca60b12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2c1eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f70e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGATDataset(RecomDataset):\n",
    "'''\n",
    "Created on Dec 18, 2018\n",
    "Tensorflow Implementation of Knowledge Graph Attention Network (KGAT) model in:\n",
    "Wang Xiang et al. KGAT: Knowledge Graph Attention Network for Recommendation. In KDD 2019.\n",
    "@author: Xiang Wang (xiangwang@u.nus.edu)\n",
    "'''\n",
    "import numpy as np\n",
    "from utility.load_data import Data\n",
    "from time import time\n",
    "import scipy.sparse as sp\n",
    "import random as rd\n",
    "import collections\n",
    "\n",
    "class KGAT_loader(Data):\n",
    "    def __init__(self, args, path):\n",
    "        super().__init__(args, path)\n",
    "\n",
    "        # generate the sparse adjacency matrices for user-item interaction & relational kg data.\n",
    "        self.adj_list, self.adj_r_list = self._get_relational_adj_list()\n",
    "\n",
    "        # generate the sparse laplacian matrices.\n",
    "        self.lap_list = self._get_relational_lap_list()\n",
    "\n",
    "        # generate the triples dictionary, key is 'head', value is '(tail, relation)'.\n",
    "        self.all_kg_dict = self._get_all_kg_dict()\n",
    "        self.exist_heads = list(self.all_kg_dict.keys())\n",
    "        \n",
    "        self.all_h_list, self.all_r_list, self.all_t_list, self.all_v_list = self._get_all_kg_data()\n",
    "\n",
    "\n",
    "    def _get_relational_adj_list(self):\n",
    "        t1 = time()\n",
    "        adj_mat_list = []\n",
    "        adj_r_list = []\n",
    "\n",
    "        def _np_mat2sp_adj(np_mat, row_pre, col_pre):\n",
    "            n_all = self.n_users + self.n_entities\n",
    "            # single-direction\n",
    "            a_rows = np_mat[:, 0] + row_pre\n",
    "            a_cols = np_mat[:, 1] + col_pre\n",
    "            a_vals = [1.] * len(a_rows)\n",
    "\n",
    "            b_rows = a_cols\n",
    "            b_cols = a_rows\n",
    "            b_vals = [1.] * len(b_rows)\n",
    "\n",
    "            a_adj = sp.coo_matrix((a_vals, (a_rows, a_cols)), shape=(n_all, n_all))\n",
    "            b_adj = sp.coo_matrix((b_vals, (b_rows, b_cols)), shape=(n_all, n_all))\n",
    "\n",
    "            return a_adj, b_adj\n",
    "\n",
    "        R, R_inv = _np_mat2sp_adj(self.train_data, row_pre=0, col_pre=self.n_users)\n",
    "        adj_mat_list.append(R)\n",
    "        adj_r_list.append(0)\n",
    "\n",
    "        adj_mat_list.append(R_inv)\n",
    "        adj_r_list.append(self.n_relations + 1)\n",
    "        print('\\tconvert ratings into adj mat done.')\n",
    "\n",
    "        for r_id in self.relation_dict.keys():\n",
    "            K, K_inv = _np_mat2sp_adj(np.array(self.relation_dict[r_id]), row_pre=self.n_users, col_pre=self.n_users)\n",
    "            adj_mat_list.append(K)\n",
    "            adj_r_list.append(r_id + 1)\n",
    "\n",
    "            adj_mat_list.append(K_inv)\n",
    "            adj_r_list.append(r_id + 2 + self.n_relations)\n",
    "        print('\\tconvert %d relational triples into adj mat done. @%.4fs' %(len(adj_mat_list), time()-t1))\n",
    "\n",
    "        self.n_relations = len(adj_r_list)\n",
    "        # print('\\tadj relation list is', adj_r_list)\n",
    "\n",
    "        return adj_mat_list, adj_r_list\n",
    "\n",
    "    def _get_relational_lap_list(self):\n",
    "        def _bi_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            bi_lap = adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt)\n",
    "            return bi_lap.tocoo()\n",
    "\n",
    "        def _si_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0.\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        if self.args.adj_type == 'bi':\n",
    "            lap_list = [_bi_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate bi-normalized adjacency matrix.')\n",
    "        else:\n",
    "            lap_list = [_si_norm_lap(adj) for adj in self.adj_list]\n",
    "            print('\\tgenerate si-normalized adjacency matrix.')\n",
    "        return lap_list\n",
    "\n",
    "    def _get_all_kg_dict(self):\n",
    "        all_kg_dict = collections.defaultdict(list)\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "\n",
    "            rows = lap.row\n",
    "            cols = lap.col\n",
    "\n",
    "            for i_id in range(len(rows)):\n",
    "                head = rows[i_id]\n",
    "                tail = cols[i_id]\n",
    "                relation = self.adj_r_list[l_id]\n",
    "\n",
    "                all_kg_dict[head].append((tail, relation))\n",
    "        return all_kg_dict\n",
    "\n",
    "    def _get_all_kg_data(self):\n",
    "        def _reorder_list(org_list, order):\n",
    "            new_list = np.array(org_list)\n",
    "            new_list = new_list[order]\n",
    "            return new_list\n",
    "\n",
    "        all_h_list, all_t_list, all_r_list = [], [], []\n",
    "        all_v_list = []\n",
    "\n",
    "        for l_id, lap in enumerate(self.lap_list):\n",
    "            all_h_list += list(lap.row)\n",
    "            all_t_list += list(lap.col)\n",
    "            all_v_list += list(lap.data)\n",
    "            all_r_list += [self.adj_r_list[l_id]] * len(lap.row)\n",
    "\n",
    "        assert len(all_h_list) == sum([len(lap.data) for lap in self.lap_list])\n",
    "\n",
    "        # resort the all_h/t/r/v_list,\n",
    "        # ... since tensorflow.sparse.softmax requires indices sorted in the canonical lexicographic order\n",
    "        print('\\treordering indices...')\n",
    "        org_h_dict = dict()\n",
    "\n",
    "        for idx, h in enumerate(all_h_list):\n",
    "            if h not in org_h_dict.keys():\n",
    "                org_h_dict[h] = [[],[],[]]\n",
    "\n",
    "            org_h_dict[h][0].append(all_t_list[idx])\n",
    "            org_h_dict[h][1].append(all_r_list[idx])\n",
    "            org_h_dict[h][2].append(all_v_list[idx])\n",
    "        print('\\treorganize all kg data done.')\n",
    "\n",
    "        sorted_h_dict = dict()\n",
    "        for h in org_h_dict.keys():\n",
    "            org_t_list, org_r_list, org_v_list = org_h_dict[h]\n",
    "            sort_t_list = np.array(org_t_list)\n",
    "            sort_order = np.argsort(sort_t_list)\n",
    "\n",
    "            sort_t_list = _reorder_list(org_t_list, sort_order)\n",
    "            sort_r_list = _reorder_list(org_r_list, sort_order)\n",
    "            sort_v_list = _reorder_list(org_v_list, sort_order)\n",
    "\n",
    "            sorted_h_dict[h] = [sort_t_list, sort_r_list, sort_v_list]\n",
    "        print('\\tsort meta-data done.')\n",
    "\n",
    "        od = collections.OrderedDict(sorted(sorted_h_dict.items()))\n",
    "        new_h_list, new_t_list, new_r_list, new_v_list = [], [], [], []\n",
    "\n",
    "        for h, vals in od.items():\n",
    "            new_h_list += [h] * len(vals[0])\n",
    "            new_t_list += list(vals[0])\n",
    "            new_r_list += list(vals[1])\n",
    "            new_v_list += list(vals[2])\n",
    "\n",
    "\n",
    "        assert sum(new_h_list) == sum(all_h_list)\n",
    "        assert sum(new_t_list) == sum(all_t_list)\n",
    "        assert sum(new_r_list) == sum(all_r_list)\n",
    "        # try:\n",
    "        #     assert sum(new_v_list) == sum(all_v_list)\n",
    "        # except Exception:\n",
    "        #     print(sum(new_v_list), '\\n')\n",
    "        #     print(sum(all_v_list), '\\n')\n",
    "        print('\\tsort all data done.')\n",
    "\n",
    "\n",
    "        return new_h_list, new_r_list, new_t_list, new_v_list\n",
    "\n",
    "    def _generate_train_A_batch(self):\n",
    "        if self.batch_size_kg <= len(self.exist_heads):\n",
    "            heads = rd.sample(exist_heads, self.batch_size_kg)\n",
    "        else:\n",
    "            heads = [rd.choice(exist_heads) for _ in range(self.batch_size_kg)]\n",
    "\n",
    "        def sample_pos_triples_for_h(h, num):\n",
    "            pos_triples = self.all_kg_dict[h]\n",
    "            n_pos_triples = len(pos_triples)\n",
    "\n",
    "            pos_rs, pos_ts = [], []\n",
    "            while True:\n",
    "                if len(pos_rs) == num: break\n",
    "                pos_id = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "\n",
    "                t = pos_triples[pos_id][0]\n",
    "                r = pos_triples[pos_id][1]\n",
    "\n",
    "                if r not in pos_rs and t not in pos_ts:\n",
    "                    pos_rs.append(r)\n",
    "                    pos_ts.append(t)\n",
    "            return pos_rs, pos_ts\n",
    "\n",
    "        def sample_neg_triples_for_h(h, r, num):\n",
    "            neg_ts = []\n",
    "            while True:\n",
    "                if len(neg_ts) == num: break\n",
    "\n",
    "                t = np.random.randint(low=0, high=self.n_users + self.n_entities, size=1)[0]\n",
    "                if (t, r) not in self.all_kg_dict[h] and t not in neg_ts:\n",
    "                    neg_ts.append(t)\n",
    "            return neg_ts\n",
    "\n",
    "        pos_r_batch, pos_t_batch, neg_t_batch = [], [], []\n",
    "\n",
    "        for h in heads:\n",
    "            pos_rs, pos_ts = sample_pos_triples_for_h(h, 1)\n",
    "            pos_r_batch += pos_rs\n",
    "            pos_t_batch += pos_ts\n",
    "\n",
    "            neg_ts = sample_neg_triples_for_h(h, pos_rs[0], 1)\n",
    "            neg_t_batch += neg_ts\n",
    "\n",
    "        return heads, pos_r_batch, pos_t_batch, neg_t_batch\n",
    "\n",
    "    def generate_train_batch(self):\n",
    "        users, pos_items, neg_items = self._generate_train_cf_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "        batch_data['users'] = users\n",
    "        batch_data['pos_items'] = pos_items\n",
    "        batch_data['neg_items'] = neg_items\n",
    "\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.users: batch_data['users'],\n",
    "            model.pos_items: batch_data['pos_items'],\n",
    "            model.neg_items: batch_data['neg_items'],\n",
    "\n",
    "            model.mess_dropout: eval(self.args.mess_dropout),\n",
    "            model.node_dropout: eval(self.args.node_dropout),\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "    def generate_train_A_batch(self):\n",
    "        heads, relations, pos_tails, neg_tails = self._generate_train_A_batch()\n",
    "\n",
    "        batch_data = {}\n",
    "\n",
    "        batch_data['heads'] = heads\n",
    "        batch_data['relations'] = relations\n",
    "        batch_data['pos_tails'] = pos_tails\n",
    "        batch_data['neg_tails'] = neg_tails\n",
    "        return batch_data\n",
    "\n",
    "    def generate_train_A_feed_dict(self, model, batch_data):\n",
    "        feed_dict = {\n",
    "            model.h: batch_data['heads'],\n",
    "            model.r: batch_data['relations'],\n",
    "            model.pos_t: batch_data['pos_tails'],\n",
    "            model.neg_t: batch_data['neg_tails'],\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n",
    "\n",
    "    def generate_test_feed_dict(self, model, user_batch, item_batch, drop_flag=True):\n",
    "\n",
    "        feed_dict ={\n",
    "            model.users: user_batch,\n",
    "            model.pos_items: item_batch,\n",
    "            model.mess_dropout: [0.] * len(eval(self.args.layer_size)),\n",
    "            model.node_dropout: [0.] * len(eval(self.args.layer_size)),\n",
    "\n",
    "        }\n",
    "\n",
    "        return feed_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243aca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af2617f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec36a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f30e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80ce2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(32,32,32), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
